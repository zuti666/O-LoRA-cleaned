import torch
from transformers import GenerationConfig
from transformers.trainer_seq2seq import Seq2SeqTrainer
from transformers.trainer import *
from transformers.trainer_callback import TrainerCallback

from uie_collator import SUPPORTED_DECODER_MODELS, check_model
from uie_dataset_lora import ANSWER_PREFIX

from optimer_sam import SAM, enable_running_stats, disable_running_stats

import h5py
import copy
from datetime import datetime 
from torch.nn import functional as F

def skip_instructions(model, predictions_ids, tokenizer, ignore_idx=-100):
    predictions_ids = np.where(predictions_ids == ignore_idx, tokenizer.pad_token_id, predictions_ids)

    predictions = tokenizer.batch_decode(
        predictions_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
    )

    final_predictions = []
    # logger.debug(f'model.config._name_or_path: {model.config._name_or_path}')
    
    if check_model(model.config._name_or_path, SUPPORTED_DECODER_MODELS):
        for pred in predictions:
            if ANSWER_PREFIX in pred:
                splits = pred.split(ANSWER_PREFIX)
                final_predictions.append(splits[-1].strip())
            else:
                final_predictions.append('')
                # final_predictions.append(pred.strip())
    else:
        final_predictions = predictions

    return final_predictions


class DenserEvalCallback(TrainerCallback):

    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):

        log_eval_steps = [1, 50, 100, 200]

        # Log
        if args.logging_strategy == IntervalStrategy.STEPS and state.global_step in log_eval_steps:
            control.should_log = True

        # Evaluate
        if args.evaluation_strategy == IntervalStrategy.STEPS and state.global_step in log_eval_steps:
            control.should_evaluate = True

        # Save
        # if args.save_strategy

        return control


class UIETrainer(Seq2SeqTrainer):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


        if getattr(self.args, "use_probe", False):
            # Â∞ùËØïÂØªÊâæ probe_head
            self.probe_head = getattr(self.model, "probe_head", None)
            if self.probe_head is None and hasattr(self.model, "base_model") and hasattr(self.model.base_model, "model"):
                self.probe_head = getattr(self.model.base_model.model, "probe_head", None)
            if self.probe_head is None:
                raise ValueError("Model does not have `probe_head` defined.")

            logger.info(f"Found probe head: {self.probe_head}")

        # if getattr(self.args, "use_probe", False):
        #     logger.info(f"Adding probe head in Trainer init")
        #     num_probe_classes = self.args.probe_num_classes
        #     hidden_size = self.model.config.hidden_size
        #     self.model.probe_head = nn.Linear(hidden_size, num_probe_classes)
        #     self.model.probe_head.to(self.args.device)  # üî• ÂÖ≥ÈîÆ‰øÆÂ§çÔºÅÊääÂàÜÁ±ªÂ§¥ÁßªÂä®Âà∞GPU

        #     # ÂÜªÁªìÂÖ∂‰ªñÂèÇÊï∞
        #     for name, param in self.model.named_parameters():
        #         if not name.startswith("probe_head"):
        #             param.requires_grad = False

    def _inner_training_loop(
        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None
    ):
        logger.info(f"Ë∞ÉÁî®Ëá™ÂÆö‰πâ looping ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉÔºåbatch_size: {batch_size}")
        self._train_batch_size = batch_size
        # Data loader and number of training steps
        train_dataloader = self.get_train_dataloader()

        # Setting up training control variables:
        # number of training epochs: num_train_epochs
        # number of training steps per epoch: num_update_steps_per_epoch
        # total number of training steps to execute: max_steps
        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size

        len_dataloader = None
        if has_length(train_dataloader):
            len_dataloader = len(train_dataloader)
            num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps
            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
            num_examples = self.num_examples(train_dataloader)
            if args.max_steps > 0:
                max_steps = args.max_steps
                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(
                    args.max_steps % num_update_steps_per_epoch > 0
                )
                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's
                # the best we can do.
                num_train_samples = args.max_steps * total_train_batch_size
            else:
                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)
                num_train_epochs = math.ceil(args.num_train_epochs)
                num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs
        elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size
            max_steps = args.max_steps
            # Setting a very large number of epochs so we go as many times as necessary over the iterator.
            num_train_epochs = sys.maxsize
            num_update_steps_per_epoch = max_steps
            num_examples = total_train_batch_size * args.max_steps
            num_train_samples = args.max_steps * total_train_batch_size
        else:
            raise ValueError(
                "args.max_steps must be set to a positive value if dataloader does not have a length, was"
                f" {args.max_steps}"
            )

        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:
            if self.args.n_gpu > 1:
                # nn.DataParallel(model) replicates the model, creating new variables and module
                # references registered here no longer work on other gpus, breaking the module
                raise ValueError(
                    "Currently --debug underflow_overflow is not supported under DP. Please use DDP"
                    " (torch.distributed.launch)."
                )
            else:
                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa

        delay_optimizer_creation = (
            self.sharded_ddp is not None
            and self.sharded_ddp != ShardedDDPOption.SIMPLE
            or is_sagemaker_mp_enabled()
            or self.fsdp is not None
        )
        if args.deepspeed:

            self.create_optimizer_and_scheduler(num_training_steps=max_steps)
            # ÁÑ∂Âêé‰∫§Áªô deepspeedÔºå‰ΩÜ‰∏çËÆ©ÂÆÉË¶ÜÁõñ optimizer
            deepspeed_engine, _, _ = deepspeed_init(
                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint
            )

            self.model = deepspeed_engine.module
            self.model_wrapped = deepspeed_engine
            self.deepspeed = deepspeed_engine



            # ---- ÂéüÊù•ÁöÑËÆæÁΩÆ
            # deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
            #     self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint
            # )
            # self.model = deepspeed_engine.module
            # self.model_wrapped = deepspeed_engine
            # self.deepspeed = deepspeed_engine
            # self.optimizer = optimizer
            # self.lr_scheduler = lr_scheduler
            #$ ---- ÂéüÊù•ÁöÑËÆæÁΩÆ

        elif not delay_optimizer_creation:
            self.create_optimizer_and_scheduler(num_training_steps=max_steps)

        self.state = TrainerState()
        self.state.is_hyper_param_search = trial is not None

        # Activate gradient checkpointing if needed
        if args.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()

        model = self._wrap_model(self.model_wrapped)

        if is_sagemaker_mp_enabled() and resume_from_checkpoint is not None:
            self._load_from_checkpoint(resume_from_checkpoint, model)

        # for the rest of this function `model` is the outside model, whether it was wrapped or not
        if model is not self.model:
            self.model_wrapped = model

        if delay_optimizer_creation:
            self.create_optimizer_and_scheduler(num_training_steps=max_steps)

        # Check if saved optimizer or scheduler states exist
        self._load_optimizer_and_scheduler(resume_from_checkpoint)

        # important: at this point:
        # self.model         is the Transformers Model
        # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model), etc.

        # Train!
        logger.info("***** Running training *****")
        logger.info(f"  Num examples = {num_examples:,}")
        logger.info(f"  Num Epochs = {num_train_epochs:,}")
        logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size:,}")
        logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}")
        logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
        logger.info(f"  Total optimization steps = {max_steps:,}")
        logger.info(f"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}")

        self.state.epoch = 0
        start_time = time.time()
        epochs_trained = 0
        steps_trained_in_current_epoch = 0
        steps_trained_progress_bar = None

        # Check if continuing training from a checkpoint
        if resume_from_checkpoint is not None and os.path.isfile(
            os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)
        ):
            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))
            epochs_trained = self.state.global_step // num_update_steps_per_epoch
            if not args.ignore_data_skip:
                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)
                steps_trained_in_current_epoch *= args.gradient_accumulation_steps
            else:
                steps_trained_in_current_epoch = 0

            logger.info("  Continuing training from checkpoint, will skip to saved global_step")
            logger.info(f"  Continuing training from epoch {epochs_trained}")
            logger.info(f"  Continuing training from global step {self.state.global_step}")
            if not args.ignore_data_skip:
                if skip_first_batches is None:
                    logger.info(
                        f"  Will skip the first {epochs_trained} epochs then the first"
                        f" {steps_trained_in_current_epoch} batches in the first epoch. If this takes a lot of time,"
                        " you can install the latest version of Accelerate with `pip install -U accelerate`.You can"
                        " also add the `--ignore_data_skip` flag to your launch command, but you will resume the"
                        " training on data already seen by your model."
                    )
                else:
                    logger.info(
                        f"  Will skip the first {epochs_trained} epochs then the first"
                        f" {steps_trained_in_current_epoch} batches in the first epoch."
                    )
                if self.is_local_process_zero() and not args.disable_tqdm and skip_first_batches is None:
                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)
                    steps_trained_progress_bar.set_description("Skipping the first batches")

        # Update the references
        self.callback_handler.model = self.model
        self.callback_handler.optimizer = self.optimizer
        self.callback_handler.lr_scheduler = self.lr_scheduler
        self.callback_handler.train_dataloader = train_dataloader
        if self.hp_name is not None and self._trial is not None:
            # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial
            # parameter to Train when using DDP.
            self.state.trial_name = self.hp_name(self._trial)
        if trial is not None:
            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial
            self.state.trial_params = hp_params(assignments)
        else:
            self.state.trial_params = None
        # This should be the same if the state has been saved but in case the training arguments changed, it's safer
        # to set this after the load.
        self.state.max_steps = max_steps
        self.state.num_train_epochs = num_train_epochs
        self.state.is_local_process_zero = self.is_local_process_zero()
        self.state.is_world_process_zero = self.is_world_process_zero()

        # tr_loss is a tensor to avoid synchronization of TPUs through .item()
        tr_loss = torch.tensor(0.0).to(args.device)
        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses
        self._total_loss_scalar = 0.0
        self._globalstep_last_logged = self.state.global_step
        model.zero_grad()

        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)

        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.
        if not args.ignore_data_skip:
            for epoch in range(epochs_trained):
                is_random_sampler = hasattr(train_dataloader, "sampler") and isinstance(
                    train_dataloader.sampler, RandomSampler
                )
                if is_torch_less_than_1_11 or not is_random_sampler:
                    # We just need to begin an iteration to create the randomization of the sampler.
                    # That was before PyTorch 1.11 however...
                    for _ in train_dataloader:
                        break
                else:
                    # Otherwise we need to call the whooooole sampler cause there is some random operation added
                    # AT THE VERY END!
                    _ = list(train_dataloader.sampler)

        total_batched_samples = 0
        for epoch in range(epochs_trained, num_train_epochs):
            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                train_dataloader.sampler.set_epoch(epoch)
            elif hasattr(train_dataloader, "dataset") and isinstance(train_dataloader.dataset, IterableDatasetShard):
                train_dataloader.dataset.set_epoch(epoch)

            if is_torch_tpu_available():
                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)
                epoch_iterator = parallel_loader
            else:
                epoch_iterator = train_dataloader

            # Reset the past mems state at the beginning of each epoch if necessary.
            if args.past_index >= 0:
                self._past = None

            steps_in_epoch = (
                len(epoch_iterator)
                if len_dataloader is not None
                else args.max_steps * args.gradient_accumulation_steps
            )
            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)

            if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:
                self._load_rng_state(resume_from_checkpoint)

            rng_to_sync = False
            steps_skipped = 0
            if skip_first_batches is not None and steps_trained_in_current_epoch > 0:
                epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)
                steps_skipped = steps_trained_in_current_epoch
                steps_trained_in_current_epoch = 0
                rng_to_sync = True

            step = -1
            for step, inputs in enumerate(epoch_iterator):
                total_batched_samples += 1
                if rng_to_sync:
                    self._load_rng_state(resume_from_checkpoint)
                    rng_to_sync = False

                # Skip past any already trained steps if resuming training
                if steps_trained_in_current_epoch > 0:
                    steps_trained_in_current_epoch -= 1
                    if steps_trained_progress_bar is not None:
                        steps_trained_progress_bar.update(1)
                    if steps_trained_in_current_epoch == 0:
                        self._load_rng_state(resume_from_checkpoint)
                    continue
                elif steps_trained_progress_bar is not None:
                    steps_trained_progress_bar.close()
                    steps_trained_progress_bar = None

                if step % args.gradient_accumulation_steps == 0:
                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)

                if (
                    (total_batched_samples % args.gradient_accumulation_steps != 0)
                    and args.local_rank != -1
                    and args._no_sync_in_gradient_accumulation
                ):
                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.
                    with model.no_sync():
                        # if self.args.use_probe:
                        if self.args.train_method == 'use_probe':
                            tr_loss_step = self.training_step(model, inputs)
                        elif self.args.train_method == "lora":
                        # ‰øÆÊîπËøõË°åÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÂú∞ÊñπÔºåÊõøÊç¢ÊéâÂéüÊù•ÁöÑÂáΩÊï∞Ë∞ÉÁî®
                        # -------------------
                            model.train()
                            enable_running_stats(model)
                            inputs = self._prepare_inputs(inputs)

                            if is_sagemaker_mp_enabled():
                                loss_mb = smp_forward_backward(
                                    model, inputs, self.args.gradient_accumulation_steps)
                                return loss_mb.reduce_mean().detach().to(self.args.device)
                            
                            # self.print_trainable_parameters(model)
                            with self.compute_loss_context_manager():
                                loss = self.compute_loss(model, inputs)

                            logger.info(f'training_step compute_loss: {loss.item():.4f}')
                            

                            if self.args.n_gpu > 1:
                                loss = loss.mean()

                            if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
                                loss = loss / self.args.gradient_accumulation_steps

                            ########################## Regularization ##########################
                            if self.args.lora_strategy.lower() == "olora":
                                orthogonal_loss = 0.
                                for name, param in model.named_parameters():
                                    if "lora_A" in name:
                                        for name_, param_ in model.named_parameters():
                                            if "loranew_A" in name_ and name.split("lora_A")[0] == name_.split("loranew_A")[0]:
                                                orthogonal_loss += torch.abs(
                                                    torch.mm(param, param_.T)).sum()
                                                break

                                l2_loss = 0.
                                for name, param in model.named_parameters():
                                    if "loranew_" in name:
                                        l2_loss += torch.norm(param, p=2)

                                lamda_1 = self.args.lamda_1
                                lamda_2 = self.args.lamda_2

                                logger.info(f"orthogonal_loss: {orthogonal_loss.item():.4f}; l2_loss: {l2_loss.item():.4f}; accuracy_loss: {loss.item():.4f}; Œª1: {lamda_1}; Œª2: {lamda_2}")

                                loss = loss + orthogonal_loss * lamda_1 + l2_loss * lamda_2
                            elif self.args.lora_strategy.lower() == "nlora":
                                l1_loss = 0.
                                loranew_A_params = {}
                                loranew_B_params = {}
                                for name, param in self.model.named_parameters():
                                    if "loranew_A" in name:
                                        loranew_A_params[name.split("loranew_A")[0]] = param
                                    elif "loranew_B" in name:
                                        loranew_B_params[name.split("loranew_B")[0]] = param

                                for key in loranew_A_params:
                                    if key in loranew_B_params:
                                        l1_loss += torch.norm(
                                            torch.mm(loranew_A_params[key], loranew_B_params[key]), p=1)

                                lamda_1 = self.args.lamda_1

                                logger.info(f"Nlora_loss: {l1_loss.item():.4f};   accuracy_loss: {loss.item():.4f}; Œª1: {lamda_1};")

                                loss = loss + l1_loss * lamda_1
                            elif self.args.lora_strategy.lower() == "inclora":
                                logger.info(f"inclora accuracy_loss: {loss.item():.4f}")
                            elif self.args.lora_strategy.lower() == "lora_l2":
                                l2_loss = 0.
                                for name, param in model.named_parameters():
                                    if "loranew_" in name:
                                        l2_loss += torch.norm(param, p=2)

                                logger.info(f" l2_loss: {l2_loss.item():.4f}; accuracy_loss: {loss.item():.4f};  Œª2: {lamda_2}")
                                lamda_2 = self.args.lamda_2
                                loss = loss + l2_loss * lamda_2
                            ######################################################################
                            logger.debug(f"sum_loss: {loss.item():.4f}")

                            if self.do_grad_scaling:
                                self.scaler.scale(loss).backward()
                            elif self.use_apex:
                                with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                                    scaled_loss.backward()
                            elif self.deepspeed:
                                # loss gets scaled under gradient_accumulation_steps in deepspeed
                                loss = self.deepspeed.backward(loss)
                            else:
                                loss.backward()

                            tr_loss_step = loss.detach()
                        #------------------------------

                else:
                    # if self.args.use_probe:
                    if self.args.train_method == 'use_probe':
                        tr_loss_step = self.training_step(model, inputs)
                    elif self.args.train_method == "lora":
                        # ‰øÆÊîπËøõË°åÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÂú∞ÊñπÔºåÊõøÊç¢ÊéâÂéüÊù•ÁöÑÂáΩÊï∞Ë∞ÉÁî®
                        # -------------------
                        model.train()
                        enable_running_stats(model)
                        inputs = self._prepare_inputs(inputs)

                        if is_sagemaker_mp_enabled():
                            loss_mb = smp_forward_backward(
                                model, inputs, self.args.gradient_accumulation_steps)
                            return loss_mb.reduce_mean().detach().to(self.args.device)
                        
                        # self.print_trainable_parameters(model)
                        with self.compute_loss_context_manager():
                            loss = self.compute_loss(model, inputs)

                        logger.info(f'training_step compute_loss: {loss.item():.4f}')
                        

                        if self.args.n_gpu > 1:
                            loss = loss.mean()

                        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
                            loss = loss / self.args.gradient_accumulation_steps

                        ########################### Regularization ##########################
                        if self.args.lora_strategy.lower() == "olora":
                            orthogonal_loss = 0.
                            for name, param in model.named_parameters():
                                if "lora_A" in name:
                                    for name_, param_ in model.named_parameters():
                                        if "loranew_A" in name_ and name.split("lora_A")[0] == name_.split("loranew_A")[0]:
                                            orthogonal_loss += torch.abs(
                                                torch.mm(param, param_.T)).sum()
                                            break

                            l2_loss = 0.
                            for name, param in model.named_parameters():
                                if "loranew_" in name:
                                    l2_loss += torch.norm(param, p=2)

                            lamda_1 = self.args.lamda_1
                            lamda_2 = self.args.lamda_2

                            logger.info(
                                f"orthogonal_loss: {orthogonal_loss.item():.4f}; l2_loss: {l2_loss.item():.4f}; accuracy_loss: {loss.item():.4f}; Œª1: {lamda_1}; Œª2: {lamda_2}")

                            loss = loss + orthogonal_loss * lamda_1 + l2_loss * lamda_2
                        elif self.args.lora_strategy.lower() == "nlora":
                            l1_loss = 0.
                            loranew_A_params = {}
                            loranew_B_params = {}
                            for name, param in self.model.named_parameters():
                                if "loranew_A" in name:
                                    loranew_A_params[name.split("loranew_A")[0]] = param
                                elif "loranew_B" in name:
                                    loranew_B_params[name.split("loranew_B")[0]] = param

                            for key in loranew_A_params:
                                if key in loranew_B_params:
                                    l1_loss += torch.norm(
                                        torch.mm(loranew_A_params[key], loranew_B_params[key]), p=1)

                            lamda_1 = self.args.lamda_1

                            logger.info(f"Nlora_loss: {l1_loss.item():.4f};   accuracy_loss: {loss.item():.4f}; Œª1: {lamda_1};")

                            loss = loss + l1_loss * lamda_1
                        elif self.args.lora_strategy.lower() == "inclora":
                            logger.info(f"inclora accuracy_loss: {loss.item():.4f}")
                        elif self.args.lora_strategy.lower() == "lora_l2":
                            l2_loss = 0.
                            for name, param in model.named_parameters():
                                if "loranew_" in name:
                                    l2_loss += torch.norm(param, p=2)

                            logger.info(f"lora_l2 l2_loss: {l2_loss.item():.4f}; accuracy_loss: {loss.item():.4f};  Œª2: {lamda_2}")
                            lamda_2 = self.args.lamda_2
                            loss = loss + l2_loss * lamda_2
                        ######################################################################
                        logger.debug(f"sum_loss: {loss.item():.4f}")
                        if self.do_grad_scaling:
                            self.scaler.scale(loss).backward()
                        elif self.use_apex:
                            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                                scaled_loss.backward()
                        elif self.deepspeed:
                            # loss gets scaled under gradient_accumulation_steps in deepspeed
                            loss = self.deepspeed.backward(loss)
                        else:
                            loss.backward()

                        tr_loss_step = loss.detach()
                    # -------------------




                if (
                    args.logging_nan_inf_filter
                    and not is_torch_tpu_available()
                    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
                ):
                    # if loss is nan or inf simply add the average of previous logged losses
                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)
                else:
                    tr_loss += tr_loss_step

                self.current_flos += float(self.floating_point_ops(inputs))

                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps
                if self.deepspeed:
                    self.deepspeed.step()


                # ÁúüÊ≠£ËøõË°åÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÂú∞Êñπ
                if total_batched_samples % args.gradient_accumulation_steps == 0 or (
                    # last step in epoch but step is always smaller than gradient_accumulation_steps
                    steps_in_epoch <= args.gradient_accumulation_steps
                    and (step + 1) == steps_in_epoch
                ):
                    # Gradient clipping
                    # ËøõË°åÊ¢ØÂ∫¶Ë£ÅÂâ™
                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:

                        logger.debug(f'args.max_grad_norm: {args.max_grad_norm}')
                        # deepspeed does its own clipping

                        if self.do_grad_scaling:
                            # Reduce gradients first for XLA
                            if is_torch_tpu_available():
                                gradients = xm._fetch_gradients(self.optimizer)
                                xm.all_reduce("sum", gradients, scale=1.0 / xm.xrt_world_size())
                            # AMP: gradients need unscaling
                            self.scaler.unscale_(self.optimizer)
                        if is_sagemaker_mp_enabled() and args.fp16:
                            self.optimizer.clip_master_grads(args.max_grad_norm)
                        elif hasattr(self.optimizer, "clip_grad_norm"):
                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping
                            self.optimizer.clip_grad_norm(args.max_grad_norm)
                        elif hasattr(model, "clip_grad_norm_"):
                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping
                            model.clip_grad_norm_(args.max_grad_norm)
                        else:
                            # Revert to normal clipping otherwise, handling Apex or full precision
                            nn.utils.clip_grad_norm_(
                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),
                                args.max_grad_norm,
                            )

                    # Optimizer step
                    optimizer_was_run = True
                    if self.deepspeed:
                        pass  # called outside the loop
                    # elif is_torch_tpu_available():
                    #     if self.do_grad_scaling:
                    #         self.scaler.step(self.optimizer)
                    #         self.scaler.update()
                    #     else:
                    #         xm.optimizer_step(self.optimizer)
                    # elif self.do_grad_scaling:
                    #     scale_before = self.scaler.get_scale()
                    #     self.scaler.step(self.optimizer)
                    #     self.scaler.update()
                    #     scale_after = self.scaler.get_scale()
                    #     optimizer_was_run = scale_before <= scale_after
                    # else:
                    #     self.optimizer.step()
                    elif hasattr(self.optimizer, "first_step") and hasattr(self.optimizer, "second_step"):
                        # === For SAM Optimizer: first_step-second_step two phases ===
                  
                        
                        # first forward-backward
                        self.optimizer.first_step(zero_grad=True)

                        # second forward-backward
                        disable_running_stats(model)
                        with self.compute_loss_context_manager():
                            second_loss = self.compute_loss(model, inputs)

                        if self.args.n_gpu > 1:
                            second_loss = second_loss.mean()
                        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
                            second_loss = second_loss / self.args.gradient_accumulation_steps

                        ########################### Regularization ##########################
                        if self.args.lora_strategy.lower() == "olora":
                            orthogonal_loss = 0.
                            for name, param in model.named_parameters():
                                if "lora_A" in name:
                                    for name_, param_ in model.named_parameters():
                                        if "loranew_A" in name_ and name.split("lora_A")[0] == name_.split("loranew_A")[0]:
                                            orthogonal_loss += torch.abs(
                                                torch.mm(param, param_.T)).sum()
                                            break

                            l2_loss = 0.
                            for name, param in model.named_parameters():
                                if "loranew_" in name:
                                    l2_loss += torch.norm(param, p=2)

                            lamda_1 = self.args.lamda_1
                            lamda_2 = self.args.lamda_2

                            logger.info(
                                f"orthogonal_loss: {orthogonal_loss.item():.4f}; l2_loss: {l2_loss.item():.4f}; accuracy_loss: {second_loss.item():.4f}; Œª1: {lamda_1}; Œª2: {lamda_2}")

                            second_loss = second_loss + orthogonal_loss * lamda_1 + l2_loss * lamda_2
                        elif self.args.lora_strategy.lower() == "nlora":
                            l1_loss = 0.
                            loranew_A_params = {}
                            loranew_B_params = {}
                            for name, param in self.model.named_parameters():
                                if "loranew_A" in name:
                                    loranew_A_params[name.split("loranew_A")[0]] = param
                                elif "loranew_B" in name:
                                    loranew_B_params[name.split("loranew_B")[0]] = param

                            for key in loranew_A_params:
                                if key in loranew_B_params:
                                    l1_loss += torch.norm(
                                        torch.mm(loranew_A_params[key], loranew_B_params[key]), p=1)

                            lamda_1 = self.args.lamda_1

                            logger.info(f"Nlora_loss: {l1_loss.item():.4f};   accuracy_loss: {second_loss.item():.4f}; Œª1: {lamda_1};")

                            second_loss = second_loss + l1_loss * lamda_1
                        elif self.args.lora_strategy.lower() == "inclora":
                            logger.info(f"inclora accuracy_loss: {second_loss.item():.4f}")
                        elif self.args.lora_strategy.lower() == "lora_l2":
                            l2_loss = 0.
                            for name, param in model.named_parameters():
                                if "loranew_" in name:
                                    l2_loss += torch.norm(param, p=2)

                            logger.info(f" l2_loss: {l2_loss.item():.4f}; accuracy_loss: {second_loss.item():.4f};  Œª2: {lamda_2}")
                            lamda_2 = self.args.lamda_2
                            second_loss = second_loss + l2_loss * lamda_2
                        ######################################################################
                        logger.info(f'SAM training_step second_loss_sum: {second_loss.item():.4f}')
                        ######################################################################

                        # Á¨¨‰∫åÊ¨°ÂèçÂêë‰º†Êí≠
                        if self.do_grad_scaling:
                            self.scaler.scale(second_loss).backward()
                            self.scaler.unscale_(self.optimizer)
                        elif self.use_apex:
                            with amp.scale_loss(second_loss, self.optimizer) as scaled_loss:
                                scaled_loss.backward()
                        elif self.deepspeed:
                            # loss gets scaled under gradient_accumulation_steps in deepspeed
                            second_loss = self.deepspeed.backward(second_loss)
                        else:
                            second_loss.backward()

                        self.optimizer.second_step(zero_grad=True)


                    else:
                    # === Standard Optimizer ===
                        if is_torch_tpu_available():
                            if self.do_grad_scaling:
                                self.scaler.step(self.optimizer)
                                self.scaler.update()
                            else:
                                xm.optimizer_step(self.optimizer)
                        elif self.do_grad_scaling:
                            scale_before = self.scaler.get_scale()
                            self.scaler.step(self.optimizer)
                            self.scaler.update()
                            scale_after = self.scaler.get_scale()
                            optimizer_was_run = scale_before <= scale_after
                        else:
                            self.optimizer.step()


                    

                    if optimizer_was_run and not self.deepspeed:
                        self.lr_scheduler.step()

                    model.zero_grad()
                    self.state.global_step += 1
                    self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch
                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)

                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
                else:
                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)

                if self.control.should_epoch_stop or self.control.should_training_stop:
                    break
            if step < 0:
                logger.warning(
                    "There seems to be not a single sample in your epoch_iterator, stopping training at step"
                    f" {self.state.global_step}! This is expected if you're using an IterableDataset and set"
                    f" num_steps ({max_steps}) higher than the number of available samples."
                )
                self.control.should_training_stop = True

            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)

            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
                if is_torch_tpu_available():
                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)
                    xm.master_print(met.metrics_report())
                else:
                    logger.warning(
                        "You enabled PyTorch/XLA debug metrics but you don't have a TPU "
                        "configured. Check your training configuration if this is unexpected."
                    )
            if self.control.should_training_stop:
                break

        if args.past_index and hasattr(self, "_past"):
            # Clean the state at the end of training
            delattr(self, "_past")

        logger.info("\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n")
        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:
            # Wait for everyone to get here so we are sur the model has been saved by process 0.
            if is_torch_tpu_available():
                xm.rendezvous("load_best_model_at_end")
            elif args.local_rank != -1:
                dist.barrier()
            elif is_sagemaker_mp_enabled():
                smp.barrier()

            self._load_best_model()

        # add remaining tr_loss
        self._total_loss_scalar += tr_loss.item()
        train_loss = self._total_loss_scalar / self.state.global_step

        metrics = speed_metrics("train", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)
        self.store_flos()
        metrics["total_flos"] = self.state.total_flos
        metrics["train_loss"] = train_loss

        self.is_in_train = False

        self._memory_tracker.stop_and_update_metrics(metrics)

        self.log(metrics)

        run_dir = self._get_output_dir(trial)
        checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)

        # Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.
        if self.args.should_save and self.state.best_model_checkpoint is not None and self.args.save_total_limit == 1:
            for checkpoint in checkpoints_sorted:
                if checkpoint != self.state.best_model_checkpoint:
                    logger.info(f"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit")
                    shutil.rmtree(checkpoint)

        self.control = self.callback_handler.on_train_end(args, self.state, self.control)

        return TrainOutput(self.state.global_step, train_loss, metrics)

    def create_optimizer_and_scheduler(self, num_training_steps: int):
        """
        Ëá™ÂÆö‰πâ‰ºòÂåñÂô®‰∏éÂ≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®ÁöÑÂàõÂª∫ÔºåÂÖºÂÆπSAM„ÄÇ
        """
        if self.optimizer is None:

            # self.scaler = torch.cuda.amp.GradScaler()
            # self.do_grad_scaling = True  # ÊéßÂà∂Ê†áÂøó
            # optim_name = self.args.optim.lower()  # <<<<<< ÊîπËøôÈáåÔºåÁªü‰∏Ä‰ªéoptimËØªÂèñ
            if self.args.optimizer_type:
                optim_name = self.args.optimizer_type.lower()
            logger.debug(
                f'Class ContinualTrainer def create_optimizer_and_scheduler() optim_name: {optim_name}')
            if optim_name == "sgd":
                self.optimizer = torch.optim.SGD(
                    self.model.parameters(), lr=self.args.learning_rate)
            elif optim_name == "sam-sgd":
                self.optimizer = SAM(
                    self.model.parameters(),
                    base_optimizer=torch.optim.SGD,
                    rho=self.args.rho,
                    lr=self.args.learning_rate,
                )
            elif optim_name == "adamw_hf":
                from transformers import AdamW
                self.optimizer = AdamW(
                    self.model.parameters(), lr=self.args.learning_rate)
            elif optim_name == "sam-adamw_hf":
                from transformers import AdamW
                self.optimizer = SAM(
                    self.model.parameters(),
                    base_optimizer=AdamW,
                    rho=self.args.rho,
                    lr=self.args.learning_rate,
                )
            elif optim_name == "adam":
                self.optimizer = torch.optim.Adam(
                    self.model.parameters(), lr=self.args.learning_rate)
            elif optim_name == "sam-adam":
                self.optimizer = SAM(
                    self.model.parameters(),
                    base_optimizer=torch.optim.Adam,
                    rho=self.args.rho,
                    lr=self.args.learning_rate,
                )

            else:
                raise ValueError(f"Unsupported optimizer type: {optim_name}")

        # if self.lr_scheduler is None:
        if 'sam' in optim_name:
            from transformers.optimization import get_constant_schedule
            self.lr_scheduler = get_constant_schedule(
                self.optimizer.base_optimizer)
            # self.lr_scheduler = LambdaLR(self.optimizer.base_optimizer, lr_lambda=lambda epoch: 1.0)
        else:
            from transformers.optimization import get_constant_schedule
            self.lr_scheduler = get_constant_schedule(self.optimizer)

            # self.lr_scheduler = LambdaLR(self.optimizer, lr_lambda=lambda epoch: 1.0)
    def print_trainable_parameters(self,model):
        total_params = 0
        trainable_params = 0
        print("Trainable parameters:")
        for name, param in model.named_parameters():
            total_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
                print(f"‚úÖ {name}: {param.shape}")
            # else:
            #     print(f"‚ùå {name}: frozen")

        print(f"\nTotal parameters: {total_params}")
        print(f"Trainable parameters: {trainable_params}")
        print(f"Trainable ratio: {100 * trainable_params / total_params:.2f}%\n")


    # def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
    #     """
    #     Ëá™ÂÆö‰πâËÆ≠ÁªÉÊ≠•È™§ÔºåÂÖºÂÆπSAM‰ºòÂåñÂô®ÂíåLoRAÊ≠£Âàô„ÄÇ
    #     """
    #     model.train()
    #     enable_running_stats(model)
    #     inputs = self._prepare_inputs(inputs)

    #     if is_sagemaker_mp_enabled():
    #         loss_mb = smp_forward_backward(
    #             model, inputs, self.args.gradient_accumulation_steps)
    #         return loss_mb.reduce_mean().detach().to(self.args.device)
        
    #     # self.print_trainable_parameters(model)
    #     with self.compute_loss_context_manager():
    #         loss = self.compute_loss(model, inputs)

    #     logger.info(f'training_step compute_loss: {loss.item():.4f}')
        

    #     if self.args.n_gpu > 1:
    #         loss = loss.mean()

    #     if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
    #         loss = loss / self.args.gradient_accumulation_steps

    #     ########################### Regularization ##########################
    #     orthogonal_loss = 0.
    #     for name, param in model.named_parameters():
    #         if "lora_A" in name:
    #             for name_, param_ in model.named_parameters():
    #                 if "loranew_A" in name_ and name.split("lora_A")[0] == name_.split("loranew_A")[0]:
    #                     orthogonal_loss += torch.abs(
    #                         torch.mm(param, param_.T)).sum()
    #                     break

    #     l2_loss = 0.
    #     for name, param in model.named_parameters():
    #         if "loranew_" in name:
    #             l2_loss += torch.norm(param, p=2)

    #     lamda_1 = self.args.lamda_1
    #     lamda_2 = self.args.lamda_2

    #     logger.info(
    #         f"orthogonal_loss: {orthogonal_loss.item():.4f}; l2_loss: {l2_loss.item():.4f}; accuracy_loss: {loss.item():.4f}; Œª1: {lamda_1}; Œª2: {lamda_2}")

    #     loss = loss + orthogonal_loss * lamda_1 + l2_loss * lamda_2



        

    #     # ÂÖàÂàÜÊîØÂ§ÑÁêÜ backward

    #     logger.debug(f'Class ContinualTrainer def training_step_sam2step()  backward')
    #     if self.do_grad_scaling:
    #         logger.debug(
    #             f'Class ContinualTrainer def training_step_()  update optim  self.do_grad_scaling{self.do_grad_scaling}')
    #         self.scaler.scale(loss).backward()
            
    #         #ÁÑ∂ÂêéËøõË°åÊ¢ØÂ∫¶Ë£ÅÂâ™ ÂíåSAM ‰ºòÂåñÂô®Êõ¥Êñ∞
    #         if is_torch_tpu_available():
    #             gradients = xm._fetch_gradients(self.optimizer)
    #             xm.all_reduce("sum", gradients, scale=1.0 / xm.xrt_world_size())
    #         self.scaler.unscale_(self.optimizer)

           



    #     elif self.use_apex: 
    #         # ËøôÈáåÁöÑ self.use_apex ÊòØÊåáÊòØÂê¶‰ΩøÁî® apex ËøõË°åÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ
    #         logger.debug(
    #             f'Class ContinualTrainer def training_step_()  update optim  self.use_apex{self.use_apex}')
    #         with amp.scale_loss(loss, self.optimizer) as scaled_loss:
    #             scaled_loss.backward()
    #     elif self.deepspeed:
    #         logger.debug(
    #             f'Class ContinualTrainer def training_step_sam2step()  update optim self.deepspeed')
    #         loss = self.deepspeed.backward(loss)
    #     else:

    #         logger.debug(
    #             f'Class ContinualTrainer def training_step  update optim else:  {self.args.optimizer_type.lower()}')
    #         loss.backward()

           

    #         if 'sam' in self.args.optimizer_type.lower():
    #             logger.debug( f'Class ContinualTrainer def training_step_sam2step()  update optim else:  rho :{self.args.rho}')
    #             # ==SAM== ÂåÖË£Ö‰∏ÄÊ≠•Êõ¥Êñ∞

    #             # def closure():
    #             #     # Ê≥®ÊÑèÔºöÊ≠§Â§Ñ zero_grad ÊòØÂøÖË¶ÅÁöÑÔºåÂõ†‰∏∫ optimizer.step(closure) È¢ÑÊúü closure ÂÜÖÈÉ®Ëá™Ë°åÊ∏ÖÁêÜÊ¢ØÂ∫¶
    #             #     self.optimizer.zero_grad()

    #             #     outputs = model(**inputs)
    #             #     loss = outputs.loss

    #             #     # Â§öÂç°Êó∂ÂèñÂπ≥Âùá
    #             #     if self.args.n_gpu > 1:
    #             #         loss = loss.mean()

    #             #     # Â¶ÇÊûúÊúâÊ¢ØÂ∫¶Á¥ØÁßØÔºåÈô§‰ª•Á¥ØÁßØÊ≠•Êï∞
    #             #     if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
    #             #         loss = loss / self.args.gradient_accumulation_steps

    #             #     # === ËøΩÂä†Ê≠£ÂàôÈ°πÔºöorthogonal loss + L2Ê≠£Âàô ===
    #             #     orthogonal_loss = 0.0
    #             #     for name, param in model.named_parameters():
    #             #         if "lora_A" in name:
    #             #             for name_, param_ in model.named_parameters():
    #             #                 if "loranew_A" in name_ and name.split("lora_A")[0] == name_.split("loranew_A")[0]:
    #             #                     orthogonal_loss += torch.abs(torch.mm(param, param_.T)).sum()
    #             #                     break

    #             #     l2_loss = 0.0
    #             #     for name, param in model.named_parameters():
    #             #         if "loranew_" in name:
    #             #             l2_loss += torch.norm(param, p=2)

    #             #     lamda_1 = getattr(self.args, "lamda_1", 0.5)
    #             #     lamda_2 = getattr(self.args, "lamda_2", 0.0)

    #             #     # Âä†Âà∞ÊÄªloss‰∏≠
    #             #     loss = loss + orthogonal_loss * lamda_1 + l2_loss * lamda_2

    #             #     # Ê†áÂáÜ backward
    #             #     loss.backward()

    #             #     return loss

    #             # # === ‰ºòÂåñÂô®ÊâßË°å closure ===
    #             # # ‚ö°ÂêØÁî® BNÂ±Ç running_statsÔºàSAMË¶ÅÊ±ÇÔºâ
    #             # enable_running_stats(model)

    #             # # ‚ö°ÂÖàÊâãÂä®ÊâßË°å‰∏ÄÊ¨° closureÔºå‰øùËØÅÁ¨¨‰∏ÄÊ¨° forward/backwardÔºåÁ¥ØÁßØÊ¢ØÂ∫¶
    #             # loss = closure()

    #             # # ‚ö°Áî± optimizer ÂÆåÊàê first_step -> second closure -> second_step
    #             # self.optimizer.step(closure=closure)

    #             # # ‚ö°Ê∏ÖÁ©∫Ê¢ØÂ∫¶
    #             # self.optimizer.zero_grad()

    #             # === SAM‰∏§Ê≠•Êõ¥Êñ∞ ===
    #             if self.args.gradient_accumulation_steps > 1 and hasattr(model, "no_sync"):
    #                 with model.no_sync():
    #                     loss.backward()
    #             else:
    #                 loss.backward()

    #             self.optimizer.first_step(zero_grad=True)

    #             # second forward-backward
    #             disable_running_stats(model)
    #             with self.compute_loss_context_manager():
    #                 second_loss = self.compute_loss(model, inputs)

    #             if self.args.n_gpu > 1:
    #                 second_loss = second_loss.mean()
    #             if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
    #                 second_loss = second_loss / self.args.gradient_accumulation_steps

    #             orthogonal_loss2 = 0.
    #             for name, param in model.named_parameters():
    #                 if "lora_A" in name: 
    #                     for name_, param_ in model.named_parameters(): 
    #                         if "loranew_A" in name_ and name.split("lora_A")[0] == name_.split("loranew_A")[0]: 
    #                             orthogonal_loss2 += torch.abs(
    #                                 torch.mm(param, param_.T)).sum()
    #                             break

    #             l2_loss2 = 0. 
    #             for name, param in model.named_parameters(): 
    #                 if "loranew_" in name: 
    #                     l2_loss2 += torch.norm(param, p=2) 

    #             logger.info( f"SAM second loss orthogonal_loss: {orthogonal_loss2.item():.4f}; l2_loss: {l2_loss2.item():.4f}; accuracy_loss: {second_loss.item():.4f}; Œª1: {lamda_1}; Œª2: {lamda_2}") 

    #             second_loss = second_loss + orthogonal_loss2 * lamda_1 + l2_loss2 * lamda_2 

    #             logger.info(f'SAM training_step second_loss_sum: {second_loss.item():.4f}')

    #             if self.args.gradient_accumulation_steps > 1 and hasattr(model, "no_sync"): 
    #                 with model.no_sync(): 
    #                     second_loss.backward() 
    #             else: 
    #                 second_loss.backward() 
    #             self.optimizer.second_step(zero_grad=True) 
 
    #         else: 
    #             loss.backward() 
    #             self.optimizer.step() 
    #             self.optimizer.zero_grad() 
        
    #     return loss.detach()
                
    

    def compute_loss(self, model, inputs, return_outputs=False):
        """
        ÂÖºÂÆπ HuggingFace Trainer ÈªòËÆ§ÊçüÂ§±ÈÄªËæë + Linear Probe Ê®°Âºè‰∏ãÁâπÂæÅÊèêÂèñ & ÂàÜÁ±ªÂ§¥ÊçüÂ§±„ÄÇ

        Linear probe Ê®°ÂºèÈÄöËøáÂèÇÊï∞ `args.use_probe` ÊéßÂà∂Ôºö
        - FalseÔºöÈªòËÆ§Ëµ∞ÂéüÂßã T5 ÊçüÂ§±Ôºàseq2seq / causal lossÔºâ
        - TrueÔºöÊõøÊç¢‰∏∫ probe_head ÂàÜÁ±ªÂô®ÊçüÂ§±ÔºàÁ∫øÊÄßÂàÜÁ±ªÂ§¥Ôºâ

        ËøòÊîØÊåÅÂ§öÁßçÁâπÂæÅÊäΩÂèñÊñπÂºèÔºàcls/eos/meanÔºâÈÄöËøá `args.probe_feature_mode` ÊéßÂà∂„ÄÇ
        """
        # --- ÈªòËÆ§ HuggingFace ÊçüÂ§±ÈÄªËæëÔºàÈùû probe Ê®°ÂºèÔºâ ---
        if not getattr(self.args, "use_probe", False):
            # üîÅ Ë∞ÉÁî®Áà∂Á±ªÈªòËÆ§ÂÆûÁé∞
            # ÈªòËÆ§Ë∑ØÂæÑÔºå‰ΩøÁî® HF ÁöÑ loss Êú∫Âà∂ÔºàÈÄÇÈÖç label_smoother„ÄÅAMP„ÄÅÂ§öÂç°Á≠âÔºâ
            logger.debug(f" run father loss")
            return super().compute_loss(model, inputs, return_outputs)
        
        else:

            # --- Linear Probe Ê®°Âºè ---
            # real_model = unwrap_model(model)

            # üî• Ê≥®ÊÑèÔºÅÔºÅÁé∞Âú®‰∏çÈúÄË¶ÅËá™Â∑±Âèñ hidden_state ÊâãÂä®Â§ÑÁêÜ‰∫Ü
            # Âè™ÈúÄË¶Å forward ËøîÂõû logitsÔºàÂú® base_model ÁöÑ forward ÈáåÂ∑≤ÁªèÂä†‰∫Ü probe_headÔºâ
            outputs = model(
                input_ids=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask", None),
                decoder_input_ids=inputs.get("decoder_input_ids", None),
                decoder_attention_mask=inputs.get("decoder_attention_mask", None),
                output_hidden_states=True,  # <<< ‚ú®‚ú®ËøôÈáåÂøÖÈ°ªÂä†‚ú®‚ú®
                return_dict=True,
            )

            logits = outputs.logits  # üî• Áõ¥Êé•Êãø logitsÔºå‰∏çÈúÄË¶ÅÊâãÂä® probe_head(features)
            class_labels = inputs["class_labels"]  # üî• Áõ¥Êé•Áî® class_labels

            # CrossEntropy Loss
            loss = F.cross_entropy(logits, class_labels.view(-1))

            return (loss, {"logits": logits, "labels": class_labels}) if return_outputs else loss
                # # üî• Ê†∏ÂøÉÔºöLinear Probe ‰ΩøÁî® features + probe_head ÂàÜÁ±ª
            # real_model =  unwrap_model(model)  # ‚úÖ Ëß£ÂåÖÔºåÊãøÂéüÂßã model

            # # logger.debug(f" run Linear Probe loss")
            # # # === Linear Probe Ê®°Âºè ===
            # # labels = inputs["labels"]
            # # decoder_input_ids = inputs.get("decoder_input_ids", None)
            # # --- ÂèñËæìÂÖ• ---
            # class_labels = inputs["class_labels"]  # ‚úÖ ‰ΩøÁî®Êï¥Êï∞ÁºñÂè∑
            # decoder_input_ids = inputs.get("decoder_input_ids", None)

            # # Forward Ëé∑Âèñ encoder/decoder Ë°®ÂæÅ
            # outputs = model(
            #     input_ids=inputs["input_ids"],
            #     attention_mask=inputs.get("attention_mask", None),
            #     decoder_input_ids=decoder_input_ids,
            #     output_hidden_states=True,
            #     return_dict=True
            # )

            # decoder_hidden_states = outputs.decoder_hidden_states[-1]  # (B, T, H)
            # decoder_attention_mask = inputs.get("decoder_attention_mask", None)

            # # --- ÁâπÂæÅÊèêÂèñÁ≠ñÁï• ---
            # mode = getattr(self.args, "probe_feature_mode", "cls")

            # if mode == "cls":
            #     # ‰ΩøÁî® decoder ÁöÑÁ¨¨‰∏Ä‰∏™ token ÁöÑ embeddingÔºàÂ∏∏Áî®‰∫é T5 Á≠âÔºâ
            #     features = decoder_hidden_states[:, 0]  # (B, H)

            # elif mode == "eos":
            #     # ‰ΩøÁî® decoder ‰∏≠ EOS token ÁöÑ embedding
            #     eos_token_id = getattr(model.config, "eos_token_id", 1)
            #     eos_mask = (decoder_input_ids == eos_token_id)  # (B, T)
            #     if eos_mask.sum(dim=1).min() == 0:
            #         raise ValueError("Some sequences do not contain an EOS token.")
            #     eos_idx = eos_mask.float().cumsum(dim=1).argmax(dim=1)  # (B,)
            #     features = decoder_hidden_states[torch.arange(decoder_hidden_states.size(0)), eos_idx]  # (B, H)

            # elif mode == "mean":
            #     # ‰ΩøÁî® decoder ÁöÑÈùû padding token Âπ≥Âùá
            #     if decoder_attention_mask is None:
            #         raise ValueError("decoder_attention_mask is required for mean pooling.")
            #     mask = decoder_attention_mask.unsqueeze(-1).float()  # (B, T, 1)
            #     summed = (decoder_hidden_states * mask).sum(dim=1)  # (B, H)
            #     count = mask.sum(dim=1).clamp(min=1e-6)  # (B, 1)
            #     features = summed / count  # (B, H)

            # else:
            #     raise ValueError(f"Unsupported probe_feature_mode: {mode}")

            # # --- Linear Probe ÂàÜÁ±ªÂ§¥ ---
            # # probe_head = getattr(model, "probe_head", None)
            # # if probe_head is None:
            # #     probe_head = getattr(model.base_model.model, "probe_head", None)
            # # if probe_head is None:
            # #     raise ValueError("Model does not have `probe_head` defined.")

            # # logits = model.probe_head(features)  # (B, num_classes)
            # # üî• Ê†∏ÂøÉÊîπÂä®ÔºöÁî® unwrapped model ÁöÑ probe_head
            # # logits = real_model.probe_head(features)
            # # loss = F.cross_entropy(logits, labels)
            # # loss = F.cross_entropy(logits, labels.view(-1))  # ‚úÖ Â§ÑÁêÜ labels shape

            
            # # # ‚úÖ Êñ∞Â¢ûÔºöÂ§ÑÁêÜ labels ÊòØÊñáÊú¨Â≠óÁ¨¶‰∏≤ÁöÑÊÉÖÂÜµ
            # # if isinstance(labels[0], str):
            # #     unique_labels = sorted(list(set(labels)))
            # #     label2id = {label: idx for idx, label in enumerate(unique_labels)}
            # #     labels = torch.tensor([label2id[label] for label in labels], dtype=torch.long, device=logits.device)

            # # elif isinstance(labels, list):
            # #     # Â¶ÇÊûúÂ∑≤ÁªèÊòØ list of int
            # #     labels = torch.tensor(labels, dtype=torch.long, device=logits.device)

            # # elif isinstance(labels, torch.Tensor):
            # #     labels = labels.to(device=logits.device, dtype=torch.long)
            # #     if labels.dim() == 2:
            # #         labels = labels[:, 0]  # ÂèñÊØèÊù° label ÁöÑÁ¨¨‰∏Ä‰∏™ token

            # # else:
            # #     raise ValueError(f"Unsupported labels type: {type(labels)}")
            
            # # # üßπ ‰øùËØÅ labels shape Ê≠£Á°ÆÔºåcross_entropy Ë¶ÅÊ±Ç (B,) ËÄå‰∏çÊòØ (B, 1)
            # # labels = labels.view(-1)
            # # loss = F.cross_entropy(logits, labels)
            # # --- CrossEntropy Loss ---

            #  # --- Probe Head ÂàÜÁ±ª ---
            # logits = real_model.probe_head(features)  # (B, num_classes)

            # # üöÄ ËÆ°ÁÆó CrossEntropy Loss
            # loss = F.cross_entropy(logits, class_labels.view(-1))  # ‚úÖ Ê≥®ÊÑè shape


            # return (loss, {"logits": logits, "labels": class_labels}) if return_outputs else loss


    # def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
    #     """
    #     Perform a training step on a batch of inputs.

    #     Subclass and override to inject custom behavior.

    #     Args:
    #         model (`nn.Module`):
    #             The model to train.
    #         inputs (`Dict[str, Union[torch.Tensor, Any]]`):
    #             The inputs and targets of the model.

    #             The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
    #             argument `labels`. Check your model's documentation for all accepted arguments.

    #     Return:
    #         `torch.Tensor`: The tensor with training loss on this batch.
    #     """
    #     model.train()
    #     inputs = self._prepare_inputs(inputs)

    #     if is_sagemaker_mp_enabled():
    #         loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)
    #         return loss_mb.reduce_mean().detach().to(self.args.device)

    #     with self.compute_loss_context_manager():
    #         loss = self.compute_loss(model, inputs)

    #     if self.args.n_gpu > 1:
    #         loss = loss.mean()  # mean() to average on multi-gpu parallel training

    #     if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
    #         # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`
    #         loss = loss / self.args.gradient_accumulation_steps

    #     ########################### Regularization ##########################
    #     orthogonal_loss = 0.
    #     for name, param in self.model.named_parameters():
    #         if "lora_A" in name:
    #             for name_, param_ in self.model.named_parameters():
    #                 if "loranew_A" in name_ and name.split("lora_A")[0] == name_.split("loranew_A")[0]:
    #                     orthogonal_loss += torch.abs(torch.mm(param, param_.T)).sum() # [r * dim] * [dim * r]
    #                     break # target modules have been matched

    #     # l2-normalization for loranew_A/B
    #     l2_loss = 0.
    #     for name, param in self.model.named_parameters():
    #         if "loranew_" in name:
    #             l2_loss += torch.norm(param, p=2)

    #     lamda_1 = self.args.lamda_1
    #     lamda_2 = self.args.lamda_2

    #     logger.info(f"orthogonal_loss: {orthogonal_loss.item()}; l2_loss: {l2_loss.item()}; accuracy_loss: {loss.item()}; Œª1: {lamda_1}; Œª2: {lamda_2}")
    #     loss = loss + orthogonal_loss * lamda_1 + l2_loss * lamda_2
    #     ######################################################################

    #     if self.do_grad_scaling:
    #         self.scaler.scale(loss).backward()
    #     elif self.use_apex:
    #         with amp.scale_loss(loss, self.optimizer) as scaled_loss:
    #             scaled_loss.backward()
    #     elif self.deepspeed:
    #         # loss gets scaled under gradient_accumulation_steps in deepspeed
    #         loss = self.deepspeed.backward(loss)
    #     else:
    #         loss.backward()

    #     return loss.detach()

    def evaluation_loop(
        self,
        dataloader: DataLoader,
        description: str,
        prediction_loss_only: Optional[bool] = None,
        ignore_keys: Optional[List[str]] = None,
        metric_key_prefix: str = "eval",
    ) -> EvalLoopOutput:
        """
        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.

        Works both with or without labels.
        """
        args = self.args

        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only
        logger.debug(
            f"Ë∞ÉÁî®Ëá™ÂÆö‰πâÂáΩÊï∞ Class ContinualTrainer def evaluation_loop self.args.predict_with_generate: {self.args.predict_with_generate},  prediction_loss_only: { prediction_loss_only},ignore_keys: {ignore_keys}"
        )


        # if eval is called w/o train init deepspeed here
        if args.deepspeed and not self.deepspeed:

            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval
            # from the checkpoint eventually
            deepspeed_engine, _, _ = deepspeed_init(
                self, num_training_steps=0, resume_from_checkpoint=None,  # inference=True
            )
            self.model = deepspeed_engine.module
            self.model_wrapped = deepspeed_engine
            self.deepspeed = deepspeed_engine

        model = self._wrap_model(self.model, training=False)

        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called
        # while ``train`` is running, cast it to the right dtype first and then put on device
        if not self.is_in_train:
            if args.fp16_full_eval:
                model = model.to(dtype=torch.float16, device=args.device)
            elif args.bf16_full_eval:
                model = model.to(dtype=torch.bfloat16, device=args.device)

        batch_size = dataloader.batch_size

        logger.info(f"***** Running {description} *****")
        if has_length(dataloader.dataset):
            logger.info(f"  Num examples = {self.num_examples(dataloader)}")
        else:
            logger.info("  Num examples: Unknown")
        logger.info(f"  Batch size = {batch_size}")

        model.eval()

        self.callback_handler.eval_dataloader = dataloader
        # Do this before wrapping.
        eval_dataset = dataloader.dataset

        if args.past_index >= 0:
            self._past = None

        # Initialize containers
        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)
        losses_host = None
        preds_host = None
        labels_host = None
        # losses/preds/labels on CPU (final containers)
        all_losses = None
        all_preds = None
        all_labels = None
        # Will be useful when we have an iterable dataset so don't know its length.

        observed_num_examples = 0
        # Main evaluation loop
        for step, inputs in enumerate(dataloader):
            # Update the observed num examples
            observed_batch_size = find_batch_size(inputs)
            if observed_batch_size is not None:
                observed_num_examples += observed_batch_size
                # For batch samplers, batch_size is not known by the dataloader in advance.
                if batch_size is None:
                    batch_size = observed_batch_size

            # Prediction step
            loss, logits, labels = self.prediction_step(
                model, inputs, prediction_loss_only, ignore_keys=ignore_keys)

            # Update containers on host
            if loss is not None:
                losses = self._nested_gather(loss.repeat(batch_size))
                losses_host = losses if losses_host is None else torch.cat(
                    (losses_host, losses), dim=0)
            if labels is not None:
                labels = self._pad_across_processes(labels)
                labels = self._nested_gather(labels)
                labels_host = labels if labels_host is None else nested_concat(
                    labels_host, labels, padding_index=-100)
            if logits is not None:
                logits = self._pad_across_processes(logits)
                logits = self._nested_gather(logits)
                if self.preprocess_logits_for_metrics is not None:
                    logits = self.preprocess_logits_for_metrics(logits, labels)
                preds_host = logits if preds_host is None else nested_concat(
                    preds_host, logits, padding_index=-100)
            self.control = self.callback_handler.on_prediction_step(
                args, self.state, self.control)

            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.
            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:
                if losses_host is not None:
                    losses = nested_numpify(losses_host)
                    all_losses = losses if all_losses is None else np.concatenate(
                        (all_losses, losses), axis=0)
                if preds_host is not None:
                    logits = nested_numpify(preds_host)
                    all_preds = logits if all_preds is None else nested_concat(
                        all_preds, logits, padding_index=-100)
                if labels_host is not None:
                    labels = nested_numpify(labels_host)
                    all_labels = (
                        labels if all_labels is None else nested_concat(
                            all_labels, labels, padding_index=-100)
                    )

                # Set back to None to begin a new accumulation
                losses_host, preds_host, labels_host = None, None, None

        if args.past_index and hasattr(self, "_past"):
            # Clean the state at the end of the evaluation loop
            delattr(self, "_past")

        # Gather all remaining tensors and put them back on the CPU
        if losses_host is not None:
            losses = nested_numpify(losses_host)
            all_losses = losses if all_losses is None else np.concatenate(
                (all_losses, losses), axis=0)
        if preds_host is not None:
            logits = nested_numpify(preds_host)
            all_preds = logits if all_preds is None else nested_concat(
                all_preds, logits, padding_index=-100)
        if labels_host is not None:
            labels = nested_numpify(labels_host)
            all_labels = labels if all_labels is None else nested_concat(
                all_labels, labels, padding_index=-100)

        # Number of samples
        if has_length(eval_dataset):
            num_samples = len(eval_dataset)
        # The instance check is weird and does not actually check for the type, but whether the dataset has the right
        # methods. Therefore we need to make sure it also has the attribute.
        elif isinstance(eval_dataset, IterableDatasetShard) and hasattr(eval_dataset, "num_examples"):
            num_samples = eval_dataset.num_examples
        else:
            num_samples = observed_num_examples

        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of
        # samplers has been rounded to a multiple of batch_size, so we truncate.
        if all_losses is not None:
            all_losses = all_losses[:num_samples]
        if all_preds is not None:
            all_preds = nested_truncate(all_preds, num_samples)
        if all_labels is not None:
            all_labels = nested_truncate(all_labels, num_samples)

        # Metrics!
        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:
            metrics = self.compute_metrics(
                dataset=eval_dataset, preds=all_preds, save_prefix=metric_key_prefix)
        else:
            metrics = {}

        metrics["global_step"] = self.state.global_step

        # To be JSON-serializable, we need to remove numpy types or zero-d tensors
        metrics = denumpify_detensorize(metrics)

        if all_losses is not None:
            metrics[f"{metric_key_prefix}_loss"] = all_losses.mean().item()

        # Prefix all keys with metric_key_prefix + '_'
        for key in list(metrics.keys()):
            if not key.startswith(f"{metric_key_prefix}_"):
                metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)

        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)

    def prediction_step(
        self,
        model: nn.Module,
        inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool,
        ignore_keys: Optional[List[str]] = None,
    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        Perform an evaluation step on `model` using `inputs`.

        Subclass and override to inject custom behavior.

        Args:
            model (`nn.Module`):
                The model to evaluate.
            inputs (`Dict[str, Union[torch.Tensor, Any]]`):
                The inputs and targets of the model.

                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
                argument `labels`. Check your model's documentation for all accepted arguments.
            prediction_loss_only (`bool`):
                Whether or not to return the loss only.

        Return:
            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and
            labels (each being optional).
        """
        # logger.debug(
        #     f"Ë∞ÉÁî®Ëá™ÂÆö‰πâÂáΩÊï∞ Class ContinualTrainer def prediction_step() self.args.predict_with_generate: {self.args.predict_with_generate},  prediction_loss_only: { prediction_loss_only},ignore_keys: {ignore_keys}"
        # )

        # üîµ Â§ÑÁêÜ Linear Probe Ê®°Âºè
        if getattr(self.args, "use_probe", False) :
           inputs = self._prepare_inputs(inputs)

           with torch.no_grad():
                outputs = model(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask", None),
                    decoder_input_ids=inputs.get("decoder_input_ids", None),
                    decoder_attention_mask=inputs.get("decoder_attention_mask", None),
                    output_hidden_states=True,
                    return_dict=True,
                )

                logits = outputs.logits  # üî• Áõ¥Êé•Êãø logits
                class_labels = inputs["class_labels"]  # üî• ‰ΩøÁî® class_labels

                loss = F.cross_entropy(logits, class_labels.view(-1))  # üî• Ê≥®ÊÑè view(-1)

                if prediction_loss_only:
                    return (loss, None, None)
                else:
                    return (loss, logits, class_labels)

       
        # Ëã•Êú™ÂêØÁî®ÁîüÊàêÂºèÈ¢ÑÊµãÔºàÂ¶ÇÂàÜÁ±ª‰ªªÂä°ÔºâÔºåÊàñËÄÖÂè™ÈúÄË¶Å lossÔºàÂ¶ÇÂè™ËÆ°ÁÆóÈ™åËØÅÊçüÂ§±ÔºâÔºåÂ∞±Áî®ÂéüÂßã Trainer ÁöÑ prediction_step
        if not self.args.predict_with_generate or prediction_loss_only:
            logger.debug(f"Ë∞ÉÁî®Áà∂Á±ª prediction_step()")
            return super().prediction_step(
                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys
            )

        has_labels = "labels" in inputs
        inputs = self._prepare_inputs(inputs)

        # XXX: adapt synced_gpus for fairscale as well
        gen_kwargs = self._gen_kwargs
        gen_kwargs["synced_gpus"] = True if is_deepspeed_zero3_enabled() else False

        if "attention_mask" in inputs:
            gen_kwargs["attention_mask"] = inputs.get("attention_mask", None)

        generation_config = GenerationConfig(**gen_kwargs)

        # prepare generation inputs
        # some encoder-decoder models can have varying encder's and thus
        # varying model input names
        # Êüê‰∫õÊ®°ÂûãÔºàÂ¶Ç encoder-decoderÔºâencoder.main_input_name ‰∏ç‰∏ÄÂÆöÊòØ 'input_ids'ÔºåÂõ†Ê≠§ËøõË°åÂÖºÂÆπÊÄßÂ§ÑÁêÜ„ÄÇ
        if hasattr(self.model, "encoder") and self.model.encoder.main_input_name != self.model.main_input_name:

            generation_inputs = inputs[self.model.encoder.main_input_name]
        else:
            generation_inputs = inputs[self.model.main_input_name]

        
        # ÁîüÊàêÂºèÈ¢ÑÊµãÊó∂Ôºålabels ‰Ωú‰∏∫ decoder ÁöÑËæìÂÖ• 
        # ‰ΩøÁî® .generate() Êù•ÁîüÊàêÈ¢ÑÊµãÊñáÊú¨Â∫èÂàó
        generated_tokens = self.model.generate(
            input_ids=generation_inputs,
            generation_config=generation_config
        )

        bs, source_len = inputs['input_ids'].shape
        # in case the batch is shorter than max length, the output should be padded
        if check_model(self.model.config._name_or_path, SUPPORTED_DECODER_MODELS):
            max_length = source_len + gen_kwargs["max_new_tokens"]
        else:
            max_length = gen_kwargs["max_new_tokens"]

        if generated_tokens.shape[-1] < max_length:
            generated_tokens = self._pad_tensors_to_max_len(
                generated_tokens, max_length)

        with torch.no_grad():
            # Â¶ÇÊûúÊúâÊ†áÁ≠æÔºåÂàôÊ≠£ÂêëÊé®ÁêÜ‰∏ÄÈÅçÁÆó loss
            if has_labels:
                with self.autocast_smart_context_manager():
                    outputs = model(**inputs)
                if self.label_smoother is not None:
                    loss = self.label_smoother(
                        outputs, inputs["labels"]).mean().detach()
                else:
                    loss = (outputs["loss"] if isinstance(
                        outputs, dict) else outputs[0]).mean().detach()
            else:
                loss = None

        # Ëã•Âè™ÈúÄË¶Å lossÔºåÂ∞±‰∏çËøîÂõûÈ¢ÑÊµãÁªìÊûú„ÄÇ
        if self.args.prediction_loss_only:
            return (loss, None, None)
        # Âê¶ÂàôËøîÂõû‰∏âÂÖÉÁªÑÔºö
        # lossÔºöÁî®‰∫éËØÑ‰º∞ÊçüÂ§±
        # generated_tokensÔºöÁî®‰∫é decode Âá∫È¢ÑÊµãÊñáÊú¨
        # labelsÔºöground truth Ê†áÁ≠æ
        

        if has_labels:
            labels = inputs["labels"]
            if labels.shape[-1] < gen_kwargs["max_new_tokens"]:
                labels = self._pad_tensors_to_max_len(
                    labels, gen_kwargs["max_new_tokens"])
        else:
            labels = None

        return (loss, generated_tokens, labels)

    # def _inner_training_loop(
    #     self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None
    # ):
    #     logger.debug(
    #         f"Ë∞ÉÁî®Ëá™ÂÆö‰πâÂáΩÊï∞ Class ContinualTrainer def _inner_training_loop() batch_size: {batch_size}, args: {args}, resume_from_checkpoint: {resume_from_checkpoint}, trial: {trial}, ignore_keys_for_eval: {ignore_keys_for_eval}"
    #     )
    #     self._train_batch_size = batch_size
    #     # Data loader and number of training steps
    #     train_dataloader = self.get_train_dataloader()

    #     # Setting up training control variables:
    #     # number of training epochs: num_train_epochs
    #     # number of training steps per epoch: num_update_steps_per_epoch
    #     # total number of training steps to execute: max_steps
    #     total_train_batch_size = args.train_batch_size * \
    #         args.gradient_accumulation_steps * args.world_size

    #     len_dataloader = None
    #     if has_length(train_dataloader):
    #         len_dataloader = len(train_dataloader)
    #         num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps
    #         num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
    #         num_examples = self.num_examples(train_dataloader)
    #         if args.max_steps > 0:
    #             max_steps = args.max_steps
    #             num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(
    #                 args.max_steps % num_update_steps_per_epoch > 0
    #             )
    #             # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's
    #             # the best we can do.
    #             num_train_samples = args.max_steps * total_train_batch_size
    #         else:
    #             max_steps = math.ceil(
    #                 args.num_train_epochs * num_update_steps_per_epoch)
    #             num_train_epochs = math.ceil(args.num_train_epochs)
    #             num_train_samples = self.num_examples(
    #                 train_dataloader) * args.num_train_epochs
    #     elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size
    #         max_steps = args.max_steps
    #         # Setting a very large number of epochs so we go as many times as necessary over the iterator.
    #         num_train_epochs = sys.maxsize
    #         num_update_steps_per_epoch = max_steps
    #         num_examples = total_train_batch_size * args.max_steps
    #         num_train_samples = args.max_steps * total_train_batch_size
    #     else:
    #         raise ValueError(
    #             "args.max_steps must be set to a positive value if dataloader does not have a length, was"
    #             f" {args.max_steps}"
    #         )

    #     if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:
    #         if self.args.n_gpu > 1:
    #             # nn.DataParallel(model) replicates the model, creating new variables and module
    #             # references registered here no longer work on other gpus, breaking the module
    #             raise ValueError(
    #                 "Currently --debug underflow_overflow is not supported under DP. Please use DDP"
    #                 " (torch.distributed.launch)."
    #             )
    #         else:
    #             debug_overflow = DebugUnderflowOverflow(self.model)  # noqa

    #     delay_optimizer_creation = (
    #         self.sharded_ddp is not None
    #         and self.sharded_ddp != ShardedDDPOption.SIMPLE
    #         or is_sagemaker_mp_enabled()
    #         or self.fsdp is not None
    #     )
    #     if args.deepspeed:

    #         # self.create_optimizer_and_scheduler(num_training_steps=max_steps)
    #         # # ÁÑ∂Âêé‰∫§Áªô deepspeedÔºå‰ΩÜ‰∏çËÆ©ÂÆÉË¶ÜÁõñ optimizer
    #         # deepspeed_engine, _, _ = deepspeed_init(
    #         #     self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint
    #         # )

    #         # self.model = deepspeed_engine.module
    #         # self.model_wrapped = deepspeed_engine
    #         # self.deepspeed = deepspeed_engine

    #         # ---- ÂéüÊù•ÁöÑËÆæÁΩÆ
    #         deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
    #             self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint
    #         )
    #         self.model = deepspeed_engine.module
    #         self.model_wrapped = deepspeed_engine
    #         self.deepspeed = deepspeed_engine
    #         self.optimizer = optimizer
    #         self.lr_scheduler = lr_scheduler
    #         # $ ---- ÂéüÊù•ÁöÑËÆæÁΩÆ

    #     elif not delay_optimizer_creation:
    #         self.create_optimizer_and_scheduler(num_training_steps=max_steps)

    #     self.state = TrainerState()
    #     self.state.is_hyper_param_search = trial is not None

    #     # Activate gradient checkpointing if needed
    #     if args.gradient_checkpointing:
    #         self.model.gradient_checkpointing_enable()

    #     model = self._wrap_model(self.model_wrapped)

    #     if is_sagemaker_mp_enabled() and resume_from_checkpoint is not None:
    #         self._load_from_checkpoint(resume_from_checkpoint, model)

    #     # for the rest of this function `model` is the outside model, whether it was wrapped or not
    #     if model is not self.model:
    #         self.model_wrapped = model

    #     if delay_optimizer_creation:
    #         self.create_optimizer_and_scheduler(num_training_steps=max_steps)

    #     # Check if saved optimizer or scheduler states exist
    #     self._load_optimizer_and_scheduler(resume_from_checkpoint)

    #     # important: at this point:
    #     # self.model         is the Transformers Model
    #     # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model), etc.

    #     # Train!
    #     logger.info("***** Running training *****")
    #     logger.info(f"  Num examples = {num_examples:,}")
    #     logger.info(f"  Num Epochs = {num_train_epochs:,}")
    #     logger.info(
    #         f"  Instantaneous batch size per device = {args.per_device_train_batch_size:,}")
    #     logger.info(
    #         f"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}")
    #     logger.info(
    #         f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    #     logger.info(f"  Total optimization steps = {max_steps:,}")
    #     logger.info(
    #         f"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}")

    #     self.state.epoch = 0
    #     start_time = time.time()
    #     epochs_trained = 0
    #     steps_trained_in_current_epoch = 0
    #     steps_trained_progress_bar = None

    #     # Check if continuing training from a checkpoint
    #     if resume_from_checkpoint is not None and os.path.isfile(
    #         os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)
    #     ):
    #         self.state = TrainerState.load_from_json(
    #             os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))
    #         epochs_trained = self.state.global_step // num_update_steps_per_epoch
    #         if not args.ignore_data_skip:
    #             steps_trained_in_current_epoch = self.state.global_step % (
    #                 num_update_steps_per_epoch)
    #             steps_trained_in_current_epoch *= args.gradient_accumulation_steps
    #         else:
    #             steps_trained_in_current_epoch = 0

    #         logger.info(
    #             "  Continuing training from checkpoint, will skip to saved global_step")
    #         logger.info(f"  Continuing training from epoch {epochs_trained}")
    #         logger.info(
    #             f"  Continuing training from global step {self.state.global_step}")
    #         if not args.ignore_data_skip:
    #             if skip_first_batches is None:
    #                 logger.info(
    #                     f"  Will skip the first {epochs_trained} epochs then the first"
    #                     f" {steps_trained_in_current_epoch} batches in the first epoch. If this takes a lot of time,"
    #                     " you can install the latest version of Accelerate with `pip install -U accelerate`.You can"
    #                     " also add the `--ignore_data_skip` flag to your launch command, but you will resume the"
    #                     " training on data already seen by your model."
    #                 )
    #             else:
    #                 logger.info(
    #                     f"  Will skip the first {epochs_trained} epochs then the first"
    #                     f" {steps_trained_in_current_epoch} batches in the first epoch."
    #                 )
    #             if self.is_local_process_zero() and not args.disable_tqdm and skip_first_batches is None:
    #                 steps_trained_progress_bar = tqdm(
    #                     total=steps_trained_in_current_epoch)
    #                 steps_trained_progress_bar.set_description(
    #                     "Skipping the first batches")

    #     # Update the references
    #     self.callback_handler.model = self.model
    #     self.callback_handler.optimizer = self.optimizer
    #     self.callback_handler.lr_scheduler = self.lr_scheduler
    #     self.callback_handler.train_dataloader = train_dataloader
    #     if self.hp_name is not None and self._trial is not None:
    #         # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial
    #         # parameter to Train when using DDP.
    #         self.state.trial_name = self.hp_name(self._trial)
    #     if trial is not None:
    #         assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial
    #         self.state.trial_params = hp_params(assignments)
    #     else:
    #         self.state.trial_params = None
    #     # This should be the same if the state has been saved but in case the training arguments changed, it's safer
    #     # to set this after the load.
    #     self.state.max_steps = max_steps
    #     self.state.num_train_epochs = num_train_epochs
    #     self.state.is_local_process_zero = self.is_local_process_zero()
    #     self.state.is_world_process_zero = self.is_world_process_zero()

    #     # tr_loss is a tensor to avoid synchronization of TPUs through .item()
    #     tr_loss = torch.tensor(0.0).to(args.device)
    #     # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses
    #     self._total_loss_scalar = 0.0
    #     self._globalstep_last_logged = self.state.global_step
    #     model.zero_grad()

    #     self.control = self.callback_handler.on_train_begin(
    #         args, self.state, self.control)

    #     # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.
    #     if not args.ignore_data_skip:
    #         for epoch in range(epochs_trained):
    #             is_random_sampler = hasattr(train_dataloader, "sampler") and isinstance(
    #                 train_dataloader.sampler, RandomSampler
    #             )
    #             if is_torch_less_than_1_11 or not is_random_sampler:
    #                 # We just need to begin an iteration to create the randomization of the sampler.
    #                 # That was before PyTorch 1.11 however...
    #                 for _ in train_dataloader:
    #                     break
    #             else:
    #                 # Otherwise we need to call the whooooole sampler cause there is some random operation added
    #                 # AT THE VERY END!
    #                 _ = list(train_dataloader.sampler)

    #     total_batched_samples = 0
    #     for epoch in range(epochs_trained, num_train_epochs):
    #         if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
    #             train_dataloader.sampler.set_epoch(epoch)
    #         elif hasattr(train_dataloader, "dataset") and isinstance(train_dataloader.dataset, IterableDatasetShard):
    #             train_dataloader.dataset.set_epoch(epoch)

    #         if is_torch_tpu_available():
    #             parallel_loader = pl.ParallelLoader(
    #                 train_dataloader, [args.device]).per_device_loader(args.device)
    #             epoch_iterator = parallel_loader
    #         else:
    #             epoch_iterator = train_dataloader

    #         # Reset the past mems state at the beginning of each epoch if necessary.
    #         if args.past_index >= 0:
    #             self._past = None

    #         steps_in_epoch = (
    #             len(epoch_iterator)
    #             if len_dataloader is not None
    #             else args.max_steps * args.gradient_accumulation_steps
    #         )
    #         self.control = self.callback_handler.on_epoch_begin(
    #             args, self.state, self.control)

    #         if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:
    #             self._load_rng_state(resume_from_checkpoint)

    #         rng_to_sync = False
    #         steps_skipped = 0
    #         if skip_first_batches is not None and steps_trained_in_current_epoch > 0:
    #             epoch_iterator = skip_first_batches(
    #                 epoch_iterator, steps_trained_in_current_epoch)
    #             steps_skipped = steps_trained_in_current_epoch
    #             steps_trained_in_current_epoch = 0
    #             rng_to_sync = True

    #         step = -1
    #         for step, inputs in enumerate(epoch_iterator):
    #             if step == 0:
    #                 logger.info("===== Sample input batch =====")
    #                 for k, v in inputs.items():
    #                     if isinstance(v, torch.Tensor):
    #                         logger.info(f"{k}: shape={v.shape}, dtype={v.dtype}")
    #                         logger.info(f"{k} sample values: {v[0][:10]}")
    #                     else:
    #                         logger.info(f"{k}: type={type(v)}")



    #             total_batched_samples += 1
    #             if rng_to_sync:
    #                 self._load_rng_state(resume_from_checkpoint)
    #                 rng_to_sync = False

    #             # Skip past any already trained steps if resuming training
    #             if steps_trained_in_current_epoch > 0:
    #                 steps_trained_in_current_epoch -= 1
    #                 if steps_trained_progress_bar is not None:
    #                     steps_trained_progress_bar.update(1)
    #                 if steps_trained_in_current_epoch == 0:
    #                     self._load_rng_state(resume_from_checkpoint)
    #                 continue
    #             elif steps_trained_progress_bar is not None:
    #                 steps_trained_progress_bar.close()
    #                 steps_trained_progress_bar = None

    #             if step % args.gradient_accumulation_steps == 0:
    #                 self.control = self.callback_handler.on_step_begin(
    #                     args, self.state, self.control)

    #             if (
    #                 (total_batched_samples %
    #                  args.gradient_accumulation_steps != 0)
    #                 and args.local_rank != -1
    #                 and args._no_sync_in_gradient_accumulation
    #             ):
    #                 # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.
    #                 with model.no_sync():
    #                     tr_loss_step = self.training_step(model, inputs)
    #             else:
    #                 tr_loss_step = self.training_step(model, inputs)

    #             if (
    #                 args.logging_nan_inf_filter
    #                 and not is_torch_tpu_available()
    #                 and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
    #             ):
    #                 # if loss is nan or inf simply add the average of previous logged losses
    #                 tr_loss += tr_loss / \
    #                     (1 + self.state.global_step - self._globalstep_last_logged)
    #             else:
    #                 tr_loss += tr_loss_step

    #             self.current_flos += float(self.floating_point_ops(inputs))

    #             # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps
    #             if self.deepspeed:
    #                 self.deepspeed.step()

    #             if total_batched_samples % args.gradient_accumulation_steps == 0 or (
    #                 # last step in epoch but step is always smaller than gradient_accumulation_steps
    #                 steps_in_epoch <= args.gradient_accumulation_steps
    #                 and (step + 1) == steps_in_epoch
    #             ):
    #                 # Gradient clipping
    #                 if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:
    #                     # deepspeed does its own clipping

    #                     if self.do_grad_scaling:
    #                         # Reduce gradients first for XLA
    #                         if is_torch_tpu_available():
    #                             gradients = xm._fetch_gradients(self.optimizer)
    #                             xm.all_reduce("sum", gradients,
    #                                           scale=1.0 / xm.xrt_world_size())
    #                         # AMP: gradients need unscaling
    #                         self.scaler.unscale_(self.optimizer)

    #                     if is_sagemaker_mp_enabled() and args.fp16:
    #                         self.optimizer.clip_master_grads(
    #                             args.max_grad_norm)
    #                     elif hasattr(self.optimizer, "clip_grad_norm"):
    #                         # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping
    #                         self.optimizer.clip_grad_norm(args.max_grad_norm)
    #                     elif hasattr(model, "clip_grad_norm_"):
    #                         # Some models (like FullyShardedDDP) have a specific way to do gradient clipping
    #                         model.clip_grad_norm_(args.max_grad_norm)
    #                     else:
    #                         # Revert to normal clipping otherwise, handling Apex or full precision
    #                         nn.utils.clip_grad_norm_(
    #                             amp.master_params(
    #                                 self.optimizer) if self.use_apex else model.parameters(),
    #                             args.max_grad_norm,
    #                         )

    #                 # Optimizer step
    #                 optimizer_was_run = True
    #                 # if self.deepspeed:
    #                 #     pass  # called outside the loop
    #                 # elif is_torch_tpu_available():
    #                 #     if self.do_grad_scaling:
    #                 #         self.scaler.step(self.optimizer)
    #                 #         self.scaler.update()
    #                 #     else:
    #                 #         xm.optimizer_step(self.optimizer)
    #                 # elif self.do_grad_scaling:
    #                 #     scale_before = self.scaler.get_scale()
    #                 #     self.scaler.step(self.optimizer)
    #                 #     self.scaler.update()
    #                 #     scale_after = self.scaler.get_scale()
    #                 #     optimizer_was_run = scale_before <= scale_after
    #                 # else:
    #                 #     self.optimizer.step()

    #                 if optimizer_was_run and not self.deepspeed:
    #                     self.lr_scheduler.step()

    #                 model.zero_grad()
    #                 self.state.global_step += 1
    #                 self.state.epoch = epoch + \
    #                     (step + 1 + steps_skipped) / steps_in_epoch
    #                 self.control = self.callback_handler.on_step_end(
    #                     args, self.state, self.control)

    #                 self._maybe_log_save_evaluate(
    #                     tr_loss, model, trial, epoch, ignore_keys_for_eval)
    #             else:
    #                 self.control = self.callback_handler.on_substep_end(
    #                     args, self.state, self.control)

    #             if self.control.should_epoch_stop or self.control.should_training_stop:
    #                 break
    #         if step < 0:
    #             logger.warning(
    #                 "There seems to be not a single sample in your epoch_iterator, stopping training at step"
    #                 f" {self.state.global_step}! This is expected if you're using an IterableDataset and set"
    #                 f" num_steps ({max_steps}) higher than the number of available samples."
    #             )
    #             self.control.should_training_stop = True

    #         self.control = self.callback_handler.on_epoch_end(
    #             args, self.state, self.control)
    #         self._maybe_log_save_evaluate(
    #             tr_loss, model, trial, epoch, ignore_keys_for_eval)

    #         if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
    #             if is_torch_tpu_available():
    #                 # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)
    #                 xm.master_print(met.metrics_report())
    #             else:
    #                 logger.warning(
    #                     "You enabled PyTorch/XLA debug metrics but you don't have a TPU "
    #                     "configured. Check your training configuration if this is unexpected."
    #                 )
    #         if self.control.should_training_stop:
    #             break

    #     if args.past_index and hasattr(self, "_past"):
    #         # Clean the state at the end of training
    #         delattr(self, "_past")

    #     logger.info(
    #         "\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n")
    #     if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:
    #         # Wait for everyone to get here so we are sur the model has been saved by process 0.
    #         if is_torch_tpu_available():
    #             xm.rendezvous("load_best_model_at_end")
    #         elif args.local_rank != -1:
    #             dist.barrier()
    #         elif is_sagemaker_mp_enabled():
    #             smp.barrier()

    #         self._load_best_model()

    #     # add remaining tr_loss
    #     self._total_loss_scalar += tr_loss.item()
    #     train_loss = self._total_loss_scalar / self.state.global_step

    #     metrics = speed_metrics(
    #         "train", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)
    #     self.store_flos()
    #     metrics["total_flos"] = self.state.total_flos
    #     metrics["train_loss"] = train_loss

    #     self.is_in_train = False

    #     self._memory_tracker.stop_and_update_metrics(metrics)

    #     self.log(metrics)

    #     run_dir = self._get_output_dir(trial)
    #     checkpoints_sorted = self._sorted_checkpoints(
    #         use_mtime=False, output_dir=run_dir)

    #     # Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.
    #     if self.args.should_save and self.state.best_model_checkpoint is not None and self.args.save_total_limit == 1:
    #         for checkpoint in checkpoints_sorted:
    #             if checkpoint != self.state.best_model_checkpoint:
    #                 logger.info(
    #                     f"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit")
    #                 shutil.rmtree(checkpoint)

    #     self.control = self.callback_handler.on_train_end(
    #         args, self.state, self.control)

    #     return TrainOutput(self.state.global_step, train_loss, metrics)

    def _compute_stats(self, eigenvalues):
        """ÊîπËøõÁÇπ13ÔºöÁªü‰∏ÄÁªüËÆ°ËÆ°ÁÆó"""
        if not eigenvalues:
            return {"min": 0., "max": 0., "median": 0., "mean": 0.}

        arr = np.array(eigenvalues)
        return {
            "min": float(np.min(arr)),
            "max": float(np.max(arr)),
            "median": float(np.median(arr)),
            "mean": float(np.mean(arr)),
            "std": float(np.std(arr))
        }

    def compute_loss_landscape(
        self, eval_dataset: Dataset, output_dir, x_range=(-1, 1), y_range=(-1, 1), num_points=10, max_batches=5, sample_batches=False,

    ):
        """
        ËÆ°ÁÆóÊçüÂ§±ÊôØËßÇÔºåÂπ∂ÂàÜÂà´ËÆ°ÁÆó:
        1Ô∏è‚É£ **ÂÆåÊï¥Ê®°ÂûãÔºàBase Model + LoRA AdapterÔºâ** ÁöÑÊçüÂ§±ÊôØËßÇ
        2Ô∏è‚É£ **‰ªÖ LoRA ÈÄÇÈÖçÂô®ÔºàLoRA AdapterÔºâ** ÁöÑÊçüÂ§±ÊôØËßÇ
        ËÆ°ÁÆóÊçüÂ§±ÊôØËßÇÔºåÂπ∂ÂàÜÂà´ËÆ°ÁÆóÂÆåÊï¥Ê®°Âûã & LoRA Adapter ÁöÑÊçüÂ§±Ë°®Èù¢„ÄÇ
        ÈíàÂØπÂ§ßÊ®°Âûã‰ΩøÁî®Â§ö GPUÔºåÂèØ‰ª•Âπ∂Ë°åÂàÜÈÖç (i, j) ÂùêÊ†áÁΩëÊ†º„ÄÇ
        """
        args = self.args
        device = args.device
        # ÊúÄÁªàÊçüÂ§±ÁΩëÁªúÁöÑÊï∞ÂÄº
        loss_grid = np.zeros((num_points, num_points))

        # Ê†πÊçÆÊ®°ÂûãÂà§Êñ≠ÔºåËøôÈáå‰∏çÂ¶®‰ΩøÁî®ÊâãÂä®
        if args.do_train:
            # Â¶ÇÊûúËøõË°åËÆ≠ÁªÉÔºåÈÇ£‰πàÊ®°ÂûãÁöÑnew_loraÂ∞±ÊòØÊñ∞ÁöÑÂΩìÂâç‰ªªÂä°Ê∑ªÂä†ÁöÑlora
            # Flag_newtaskLoRA = ''  # Â¶ÇÊûúÂè™Âπ≤Êâ∞ËÆ≠ÁªÉÂêéÁöÑÊñ∞ÁöÑ‰ªªÂä°ÁöÑlora
            # Flag_onlyLoRA = ''  # Â¶ÇÊûúÂè™Âπ≤Êâ∞lora ÈÉ®ÂàÜ
            # Flag_fullModel =  'fullModel'  # Â¶ÇÊûúÂπ≤Êâ∞Ê®°ÂûãÁöÑÊâÄÊúâÂèÇÊï∞
            distrub_name = 'fullModel'
            surf_file = os.path.join(
                    output_dir, f"{args.lora_strategy}_{distrub_name}_T5large_testData.h5")
        else:
            distrub_name = 'fullModel'
            # Â¶ÇÊûú‰∏çËøõË°åËÆ≠ÁªÉÔºåÂè™Â±ïÁ§∫Âä†ËΩΩÊ®°ÂûãÁöÑ lossland ÔºåÂàôÂØπÊ®°ÂûãÁöÑÊâÄÊúâÂèÇÊï∞(‰∏çÂåÖÊã¨new_lora)ÈÉΩËøõË°åÂπ≤Êâ∞Ôºå
            surf_file = os.path.join(output_dir, f"{distrub_name}_T5large_testData.h5")


        # Á°Æ‰øùÁõÆÊ†áÁõÆÂΩïÂ≠òÂú®ÔºàËã•‰∏çÂ≠òÂú®ÂàôÂàõÂª∫Ôºâ
        if not os.path.exists(surf_file):
            with open(surf_file, 'w') as f:
                pass  # ÊàêÂäüÂàõÂª∫Á©∫Êñá‰ª∂

        # ‚úÖ ËÆ∞ÂΩïÊó•Âøó‰ø°ÊÅØ
        logger.info(f'***5***--5-2 **1 compute_loss_landscape  ')
        logger.info(
            f"***** Running Loss Landscape Calculati on {distrub_name}*****")
        logger.info(
            f"Output Dir = {output_dir}ÔºåOutput File :{surf_file} Num points = {num_points}x{num_points} max_batches = {max_batches}")

        # ‚úÖ ÂÖºÂÆπ AMP ÂíåÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ
        # Â§çÂà∂Ê®°ÂûãÈÅøÂÖçÊ±°Êüì
        model = copy.deepcopy(self.model)
        model = self._wrap_model(model, training=False)

        # ‚úÖ Â§ÑÁêÜ FP16/BF16 ËØÑ‰º∞Ê®°Âºè
        if not self.is_in_train:
            if args.fp16_full_eval:
                model = model.to(dtype=torch.float16, device=device)
            elif args.bf16_full_eval:
                model = model.to(dtype=torch.bfloat16, device=device)

        model = model.to(device=device)
        model.eval()  # Á°Æ‰øùÊ®°ÂûãÂú® eval Ê®°Âºè

        # --------------------- ÁîüÊàêÂèÇÊï∞Êâ∞Âä®Èò∂ÊÆµÈò∂ÊÆµ ---------------------
        # Á°ÆÂÆöÈúÄË¶ÅÊâ∞Âä®ÁöÑÂèÇÊï∞ÂêçÁß∞
        # Ê†πÊçÆ‰∏çÂêåÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºàNloraÊàñloraÔºâÁ°ÆÂÆöÈúÄË¶Å‰øùÂ≠òÂéüÂßãÂÄºÁöÑÂèÇÊï∞
        original_params_to_perturb = {}
        for name, param in model.named_parameters():
            if args.do_train:
                if distrub_name == 'fullModel':
                    pass
            else:# ‰∏çÊòØËÆ≠ÁªÉÂêéÁõ¥Êé•ËØÑ‰º∞ÔºåÂä†ËΩΩÁöÑÊ®°Âûã‰∏çÂ∫îËØ•ÂåÖÊã¨Êñ∞Ê∑ªÂä†ÁöÑ new_lora ÈÉ®ÂàÜ
                if distrub_name == 'fullModel':
                    if "new_lora" not in name : 
                        original_params_to_perturb[name] = param.data.clone()

        # --------------------- Êâ∞Âä®ÁîüÊàê‰ºòÂåñ ---------------------
        torch.manual_seed(42)  # Âõ∫ÂÆöÈöèÊú∫ÁßçÂ≠ê‰øùËØÅÂèØÈáçÂ§çÊÄß
        perturb_x, perturb_y = {}, {}
        with torch.no_grad():
            for name, param in model.named_parameters():
                if name not in original_params_to_perturb:
                    continue

                # Áõ¥Êé•ÁîüÊàêÂΩí‰∏ÄÂåñÊâ∞Âä®Ôºà‰ºòÂåñÁÇπ2Ôºâ
                # norm_factor = torch.norm(param) + 1e-8  # ËÆ°ÁÆóÁ•ûÁªèÁΩëÁªúÂèÇÊï∞ÁöÑÂ∞∫Â∫¶
                # torch.manual_seed(seed_x)
                # d_x_perturb = torch.randn_like(param).to(device)  # ÁîüÊàêÈöèÊú∫Êâ∞Âä®
                # perturb_x[name] = (d_x_perturb / torch.norm(d_x_perturb)) * norm_factor  # ÂΩí‰∏ÄÂåñÂπ∂Ë∞ÉÊï¥Â∞∫Â∫¶
                else:
                    d_x = torch.randn_like(param)
                    d_x = (d_x / d_x.norm()) * (param.norm() + 1e-8)  # Áõ¥Êé•ÂΩí‰∏ÄÂåñ
                    perturb_x[name] = d_x.to(device)

                    # ÁîüÊàêÊ≠£‰∫§Êâ∞Âä®
                    d_y = torch.randn_like(param)
                    d_y = d_y - torch.sum(d_y * d_x) * \
                        d_x / (d_x.norm()**2)  # ÊñΩÂØÜÁâπÊ≠£‰∫§Âåñ
                    d_y = (d_y / d_y.norm()) * (param.norm() + 1e-8)     # ÂΩí‰∏ÄÂåñ
                    perturb_y[name] = d_y.to(device)
        # ‰ΩøÁî®FP16Â≠òÂÇ®Êâ∞Âä®Ôºà‰ºòÂåñÁÇπ5Ôºâ
        if args.fp16_full_eval or args.bf16_full_eval:
            perturb_x = {k: v.half() for k, v in perturb_x.items()}
            perturb_y = {k: v.half() for k, v in perturb_y.items()}

        # --------------------- Âπ∂Ë°åÂåñÁΩëÊ†ºËÆ°ÁÆóÔºà‰ºòÂåñÁÇπ3Ôºâ---------------------
        x_coords = np.linspace(x_range[0], x_range[1], num_points)
        y_coords = np.linspace(y_range[0], y_range[1], num_points)

        # ÂàÜÂ∏ÉÂºè‰ªªÂä°ÂàíÂàÜ
        if torch.distributed.is_initialized():
            world_size = torch.distributed.get_world_size()  # Ëé∑ÂèñÊÄªËøõÁ®ãÊï∞ÔºàGPUÊï∞ÈáèÔºâ
            rank = torch.distributed.get_rank()  # Ëé∑ÂèñÂΩìÂâçËøõÁ®ãÁöÑÁºñÂè∑Ôºà‰ªé0ÂºÄÂßãÔºâ
            # ÊåâË°åÂàíÂàÜ‰ªªÂä°
            chunk = num_points // world_size
            start_idx = rank * chunk
            end_idx = (rank + 1) * chunk if rank != world_size - \
                1 else num_points
            # all_batches = all_batches[rank::world_size]  # Êï∞ÊçÆÂàÜÁâá
            logger.info(
                f'***5***--5-2**4 distribute--yes world_size:{world_size} rank:{rank},start_idx:{start_idx},end_idx:{end_idx} ')
        else:
            # logger.info(f'****5***--5-2**2-1 if**5***--5-2**5 distribute--no ')
            world_size = 1
            rank = 0
            start_idx, end_idx = 0, num_points
            logger.info(
                f'***5***--5-2**4 distribute--no world_size:{world_size} rank:{rank},start_idx:{start_idx},end_idx:{end_idx} ')

        # --------------------- Êï∞ÊçÆÂáÜÂ§áÈò∂ÊÆµ ---------------------
        logger.info(
            f'***5***--5-2**2 begin load data---{torch.distributed.is_initialized()} ')
        dataloader = self.get_eval_dataloader(eval_dataset)
        all_batches = list(dataloader)[:max_batches]
        logger.info(f'***5***--5-2**2 begin load data---{len(all_batches)} ')

        # ÂêàÂπ∂ÊâÄÊúâÊâπÊ¨°‰∏∫Âçï‰∏™Â§ßÊâπÊ¨°ÔºàÊòæÂ≠òÂÖÅËÆ∏Êó∂Ôºâ
        if not sample_batches and len(all_batches) > 0:
            try:
                # Â∞ùËØïÂêàÂπ∂ÊâÄÊúâÂ∞èÊâπÊ¨°‰∏∫‰∏Ä‰∏™Â§ßÊâπÊ¨°
                big_batch = {
                    # ÂØπÊØè‰∏™ÁâπÂæÅÈîÆÔºàÂ¶Çinput_ids„ÄÅlabelsÔºâËøõË°åÁ∫µÂêëÊãºÊé•
                    k: torch.cat([b[k] for b in all_batches], dim=0)
                    for k in all_batches[0].keys()  # ÂÅáËÆæÊâÄÊúâÊâπÊ¨°ÁªìÊûÑÁõ∏Âêå
                }
                # Áî®ÂêàÂπ∂ÂêéÁöÑÂ§ßÊâπÊ¨°ÊõøÊç¢ÂéüÂßãÊâπÊ¨°ÂàóË°®
                all_batches = [big_batch]  # Áé∞Âú®Âè™ÂåÖÂê´‰∏Ä‰∏™ÂêàÂπ∂ÂêéÁöÑÊâπÊ¨°
            except RuntimeError:
                # ÊòæÂ≠ò‰∏çË∂≥Êó∂ÂõûÈÄÄÂà∞ÂéüÂßãÂ∞èÊâπÊ¨°
                logger.warning("Êó†Ê≥ïÂêàÂπ∂ÊâπÊ¨°Ôºå‰øùÊåÅÂéüÊúâÊâπÊ¨°Êï∞Èáè")

# --------------------- ‰∏ªËÆ°ÁÆóÂæ™ÁéØ‰ºòÂåñ ---------------------

        for i in tqdm(range(start_idx, end_idx), desc=f"Rank {rank} Processing"):
            xv = x_coords[i]
            for j, yv in enumerate(y_coords):
                # ÊÅ¢Â§çÂèÇÊï∞Êó∂‰ªÖÊìç‰ΩúÈúÄË¶Å‰øÆÊîπÁöÑÈÉ®ÂàÜÔºà‰ºòÂåñÁÇπ1Ôºâ
                for name in original_params_to_perturb:
                    model.state_dict()[name].copy_(
                        original_params_to_perturb[name])

                # Â∫îÁî®Êâ∞Âä®
                with torch.no_grad():
                    for name in original_params_to_perturb:
                        param = model.state_dict()[name]
                        delta = xv * \
                            perturb_x[name].to(
                                param.dtype) + yv * perturb_y[name].to(param.dtype)
                        param.add_(delta)  # Âéü‰ΩçÊìç‰ΩúÂáèÂ∞ëÂÜÖÂ≠òÂàÜÈÖç

                # ËÆ°ÁÆóÊçüÂ§±
                total_loss = 0.0
                for batch in all_batches:
                    inputs = {k: v.to(device) for k, v in batch.items()}
                    with torch.cuda.amp.autocast(enabled=args.fp16):  # ÊîØÊåÅÊ∑∑ÂêàÁ≤æÂ∫¶
                        outputs = model(**inputs)
                        loss = F.cross_entropy(
                            outputs.logits.view(-1, outputs.logits.size(-1)),
                            inputs["labels"].view(-1)
                        )
                    total_loss += loss.item()

                loss_grid[i, j] = total_loss / len(all_batches)

                # ÊØè5Ê¨°Ëø≠‰ª£Ê∏ÖÁêÜ‰∏ÄÊ¨°ÁºìÂ≠òÔºà‰ºòÂåñÁÇπ6Ôºâ
                if j % 5 == 0:
                    torch.cuda.empty_cache()

        # --------------------- ÂàÜÂ∏ÉÂºèÁªìÊûúÊî∂ÈõÜ ---------------------
        if torch.distributed.is_initialized():
            # Êî∂ÈõÜÊâÄÊúâËøõÁ®ãÁöÑloss_grid
            # Â∞Ü loss_grid ËΩ¨Êç¢‰∏∫Âº†Èáè
            loss_grid_tensor = torch.tensor(loss_grid, device=device)
            # ÂàõÂª∫‰∏Ä‰∏™ÂàóË°®ÔºåÁî®‰∫éÊé•Êî∂ÊâÄÊúâËøõÁ®ãÁöÑ loss_grid
            all_loss = [torch.zeros_like(loss_grid_tensor)
                        for _ in range(world_size)]
            # Êî∂ÈõÜÊâÄÊúâËøõÁ®ãÁöÑ loss_grid
            torch.distributed.all_gather(all_loss, loss_grid_tensor)
            # Â∞ÜÊî∂ÈõÜÂà∞ÁöÑÁªìÊûúÊãºÊé•Êàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ loss_grid
            # loss_grid_tensor = torch.cat(all_loss, dim=0)
            loss_grid_tensor = torch.stack(all_loss, dim=0).mean(dim=0)
            loss_grid = loss_grid_tensor.cpu().numpy()  # ËΩ¨Âõû NumPy Êï∞ÁªÑÂêéÂÜç‰øùÂ≠ò
        else:
            all_loss = [loss_grid]

        # ‰ªÖrank 0ËøõÁ®ã‰øùÂ≠òÁªìÊûú
        if rank == 0:
            with h5py.File(surf_file, 'w') as f:
                f.create_dataset('xcoordinates', data=x_coords)
                f.create_dataset('ycoordinates', data=y_coords)
                f.create_dataset('train_loss', data=loss_grid)

            logger.info(f"ËÆ°ÁÆóÂÆåÊàêÔºåÁªìÊûú‰øùÂ≠òËá≥{surf_file}")

        # return True
    


    def compute_fisher_information(self, eval_dataset, output_dir, name="FisherInfo", fisher_samples=1000, batch_size=32):
        """
        ËÆ°ÁÆó Fisher Information Âπ∂Â≠òÂÇ®Âà∞ HDF5 Êñá‰ª∂ÔºåÂåÖÊã¨ÁªüËÆ°‰ø°ÊÅØ
        :param flag_Nlora: ÊòØÂê¶‰ΩøÁî® Nlora ÊñπÊ≥ï
        :param eval_dataset: ËØÑ‰º∞Êï∞ÊçÆÈõÜ
        :param output_dir: ÁªìÊûúÂ≠òÂÇ®ÁõÆÂΩï
        :param name: ÁªìÊûúÊñá‰ª∂ÂêçÁß∞
        :param fisher_samples: ËÆ°ÁÆó Fisher ‰ø°ÊÅØÊâÄÁî®ÁöÑÊ†∑Êú¨Êï∞
        :param batch_size: ËÆ°ÁÆó Fisher ‰ø°ÊÅØÊó∂ÁöÑ batch Â§ßÂ∞è
        """
        args = self.args
        device = args.device

        if args.lora_strategy.lower() == 'nlora':
            fisher_file = os.path.join(
                output_dir, f"{name}_Fisher_Nlora_task.h5")
        elif args.lora_strategy.lower() == 'lora':
            fisher_file = os.path.join(output_dir, f"{name}_Fisher_LoRA.h5")

        os.makedirs(output_dir, exist_ok=True)

        # Â§çÂà∂Ê®°ÂûãÈÅøÂÖçÊ±°Êüì
        model = copy.deepcopy(self.model)
        model = self._wrap_model(model, training=False)
        model.to(device).eval()

        # ÈöèÊú∫ÈááÊ†∑ËÆ≠ÁªÉÊ†∑Êú¨
        sampler = RandomSampler(
            eval_dataset, replacement=True, num_samples=fisher_samples)
        dataloader = DataLoader(
            eval_dataset, sampler=sampler, batch_size=batch_size)

        fisher_vector = None
        total_samples = 0

        logger.info(
            f"ËÆ°ÁÆó Fisher Information, Ê†∑Êú¨Êï∞: {fisher_samples}, batch_size: {batch_size}")

        for batch in tqdm(dataloader, desc="Computing Fisher Information"):
            inputs, labels = batch
            inputs, labels = inputs.to(device), labels.to(device)

            model.zero_grad()

            # ËÆ°ÁÆóÊ®°ÂûãËæìÂá∫
            outputs = model(inputs)
            log_probs = F.log_softmax(outputs.logits, dim=-1)

            # ÈÄâÊã© ground-truth ÂØπÂ∫îÁöÑ log Ê¶ÇÁéá
            log_probs_selected = log_probs.gather(
                dim=-1, index=labels.unsqueeze(-1)).squeeze()

            # ËÆ°ÁÆóÂØπÊï∞Ê¶ÇÁéáÁöÑÊ¢ØÂ∫¶
            gradients = torch.autograd.grad(
                log_probs_selected.sum(), model.parameters(), create_graph=False)

            # ËÆ°ÁÆó Fisher Information ÁöÑÂØπËßíËøë‰ºº
            grad_vector = torch.cat([g.view(-1)
                                    for g in gradients]).detach() ** 2

            # Fisher ‰ø°ÊÅØÁ¥ØÂä†
            if fisher_vector is None:
                fisher_vector = grad_vector
            else:
                fisher_vector += grad_vector

            total_samples += len(labels)

            # ÊØè 5 ‰∏™ batch ÈáäÊîæÊòæÂ≠ò
            if total_samples % (5 * batch_size) == 0:
                torch.cuda.empty_cache()

        fisher_vector /= total_samples  # ÂΩí‰∏ÄÂåñ

        # **ËÆ°ÁÆó Fisher Information ÁªüËÆ°‰ø°ÊÅØ**
        fisher_stats = self._compute_fisher_stats(fisher_vector.cpu().numpy())

        # **‰øùÂ≠ò Fisher ‰ø°ÊÅØÂíåÁªüËÆ°‰ø°ÊÅØ**
        with h5py.File(fisher_file, 'w') as f:
            f.create_dataset('fisher_vector', data=fisher_vector.cpu().numpy())
            for k, v in fisher_stats.items():
                f.attrs[k] = v  # Áõ¥Êé•Â∞ÜÁªüËÆ°Êï∞ÊçÆÂÜôÂÖ• HDF5 Êñá‰ª∂

        logger.info(f"Fisher Information ËÆ°ÁÆóÂÆåÊàêÔºåÁªìÊûú‰øùÂ≠òËá≥ {fisher_file}")
        logger.info(f"Fisher ÁªüËÆ°‰ø°ÊÅØ: {fisher_stats}")

    def _compute_fisher_stats(self, fisher_values):
        """ËÆ°ÁÆó Fisher Information ÁöÑÁªüËÆ°‰ø°ÊÅØ"""
        if fisher_values is None or len(fisher_values) == 0:
            return {"min": 0., "max": 0., "median": 0., "mean": 0., "std": 0.}

        arr = np.array(fisher_values)
        return {
            "min": float(np.min(arr)),
            "max": float(np.max(arr)),
            "median": float(np.median(arr)),
            "mean": float(np.mean(arr)),
            "std": float(np.std(arr))
        }

    def compute_hessian_version1(
        self,
        flag_Nlora,
        eval_dataset,
        output_dir,
        name="hessian",
        max_batches=10,
        sample_batches=False,
        use_gpu=True,
        flag_Nlora_full=True,

    ):
        """
        ÊîπËøõÁâàHessianÁü©ÈòµËÆ°ÁÆóÂáΩÊï∞Ôºå‰∏ªË¶Å‰ºòÂåñÔºö
        1. ÂÆåÂÖ®Ê∂àÈô§ÂèÇÊï∞Ê±°ÊüìÈ£éÈô©
        2. ÊîØÊåÅÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁéØÂ¢É
        3. Â¢ûÂº∫Êï∞ÂÄºÁ®≥ÂÆöÊÄß
        4. ÂÜÖÂ≠òÊïàÁéá‰ºòÂåñ
        5. Â¢ûÂä†ÁâπÂæÅÂêëÈáèÂàÜÊûê

        ÂèÇÊï∞ËØ¥ÊòéÔºö
        - max_batches: ÊúÄÂ§ßËÆ°ÁÆóbatchÊï∞ÔºàÁî®‰∫éÂ§ßÊï∞ÊçÆÈõÜÈááÊ†∑Ôºâ
        - sample_batches: ÊòØÂê¶ÈöèÊú∫ÈááÊ†∑batchÔºàTrue=ÈöèÊú∫ÔºåFalse=È°∫Â∫èÂèñÂâçN‰∏™Ôºâ

        """
        # --------------------- ÂàùÂßãÂåñÈò∂ÊÆµ ---------------------
        # Ê†πÊçÆÊ®°ÂûãÂà§Êñ≠ÔºåËøôÈáå‰∏çÂ¶®‰ΩøÁî®ÊâãÂä®

        if flag_Nlora and (not flag_Nlora_full):
            # Â¶ÇÊûúÊòØ Nlora ÊñπÊ≥ïÔºåÂè™Âπ≤Êâ∞ task Áõ∏ÂÖ≥ÁöÑ‰ª£Á†Å
            Flag_Nlora_newtask = True
            Flag_Nlora_full = False
            Flag_lora = False
        elif flag_Nlora and flag_Nlora_full:
            # Â¶ÇÊûúÊòØ Nlora ÊñπÊ≥ïÔºåÂπ≤Êâ∞ task_lora Âíå‰πãÂâçÁöÑlora Áõ∏ÂÖ≥ÁöÑ‰ª£Á†Å
            Flag_Nlora_newtask = True  # Âè™Ë¶ÅÊòØ‰ΩøÁî®‰∫ÜNloraÔºåËøôÈáåÂ∞±ÊòØTrue
            Flag_Nlora_full = True  # Ëøô‰∏™ÊòØÁî®Êù•ÂîØ‰∏ÄÂå∫Âà´ full ËøòÊòØ task
            Flag_lora = False
        else:
            Flag_Nlora_newtask = False
            Flag_Nlora_full = False
            Flag_lora = True

        logger.info(f'***5***--5-3**1 begin init   ')
        logger.info(
            f'***5***--5-3**1 use distribute ---- {torch.distributed.is_initialized()}')
        args = self.args
        device = args.device

        hessian_file_Nlora_fulllora = os.path.join(
            output_dir, f"{name}_Nlora_only-predictDataset.h5")
        hessian_file_Nlora_tasklora = os.path.join(
            output_dir, f"{name}_Nlora_only-predictDataset.h5")
        hessian_file_lora = os.path.join(
            output_dir, f"{name}_lora_only-predictDataset_lanczos.h5")

        # ÂàõÂª∫Áã¨Á´ãÊ®°ÂûãÂâØÊú¨ÔºàÂÖ≥ÈîÆÊîπËøõÁÇπ1ÔºöÈöîÁ¶ªÂéüÂßãÊ®°ÂûãÔºâ
        model = copy.deepcopy(self.model)
        model = self._wrap_model(model, training=False)

        # Ê∑∑ÂêàÁ≤æÂ∫¶Â§ÑÁêÜ
        if not self.is_in_train:
            if args.fp16_full_eval:
                model = model.to(dtype=torch.float16, device=device)
            elif args.bf16_full_eval:
                model = model.to(dtype=torch.bfloat16, device=device)

        model = model.to(device=device)
        model.eval()

        # ‰øùÂ≠òÂàùÂßãÂèÇÊï∞Áä∂ÊÄÅÔºàÂÖ≥ÈîÆÊîπËøõÁÇπ2ÔºöÊ∂àÈô§ÂèÇÊï∞Ê±°ÊüìÔºâ
        # original_state = {
        # k: v.to(device) if isinstance(v, torch.Tensor) else v
        # for k, v in model.state_dict().items()
        # }
        # original_state = copy.deepcopy(model.state_dict())
        logger.info(f'***5***--LORA Hessian**1 finish init ')

        # --------------------- Êï∞ÊçÆÂáÜÂ§áÈò∂ÊÆµ ---------------------
        logger.info(f'***5***--5-3**2 begin load data   ')
        dataloader = self.get_eval_dataloader(eval_dataset)
        all_batches = list(dataloader)

        if torch.distributed.is_initialized():
            world_size = torch.distributed.get_world_size()
            rank = torch.distributed.get_rank()
            all_batches = all_batches[rank::world_size]  # Êï∞ÊçÆÂàÜÁâá
            logger.info(
                f'***5***--LORA Hessian**2 distribute yes, rank:{rank}, total_batches:{len(all_batches)}')

        else:
            logger.info(f'***5***--5-3**4 distribute--no ')
            world_size = 1
            rank = 0

        # ÊâπÈáèÈááÊ†∑ÈÄªËæëÔºàÊîπËøõÁÇπ3ÔºöÂÜÖÂ≠ò‰ºòÂåñÔºâ
        logger.info(
            f'***5***--5-3**2-1 if  {len(all_batches)} , {max_batches} ')
        if len(all_batches) > max_batches:
            if sample_batches:
                indices = np.random.choice(
                    len(all_batches), max_batches, replace=False)
                all_batches = [all_batches[i] for i in indices]
            else:
                all_batches = all_batches[:max_batches]

        logger.info(f'***5***--5-3**2 finish load data ')
        # --------------------- Ê†∏ÂøÉÁÆóÊ≥ïÂÆö‰πâ ---------------------

        class HessianCalculator:
            """
            Hessian ËÆ°ÁÆóÂô®Ôºö
            - ËÆ°ÁÆó Hessian-Vector Product (HVP)
            - ‰ΩøÁî® Lanczos ÊñπÊ≥ï‰º∞ËÆ° Hessian ÁöÑÁâπÂæÅÂÄº

            ÂèÇÊï∞Ôºö
            - model: ËÆ°ÁÆó Hessian ÁöÑÁ•ûÁªèÁΩëÁªúÊ®°Âûã
            - device: ËÆ°ÁÆóËÆæÂ§á (CPU/GPU)
            """

            def __init__(self, model, device, max_dim=10000):
                logger.info(f'***5***--5-3**3 class HessianCalculator init   ')
                self.model = model
                self.device = device
                self.criterion = torch.nn.CrossEntropyLoss()
                # self.max_dim = max_dim  # ÈôêÂà∂ Hessian ËÆ°ÁÆóÁöÑÊúÄÂ§ßÁª¥Â∫¶

            @staticmethod
            def _safe_normalize(v, eps=1e-12):
                """ÊîπËøõÁÇπ4ÔºöÂÆâÂÖ®ÂΩí‰∏ÄÂåñÈò≤Ê≠¢Èô§Èõ∂ÈîôËØØ"""
                norm = torch.norm(v) + eps
                return v / norm

            def compute_hvp(self, batch, param_list=None):
                """ËÆ°ÁÆóHessian-vector‰πòÁßØÂáΩÊï∞ (HVP) """
                logger.info(f'***5***--5-3**5 begin compute_hvp() ')
                self.model.zero_grad()  # Ê∏ÖÁ©∫Ê¢ØÂ∫¶
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with torch.autograd.set_grad_enabled(True):
                    # outputs = checkpoint(self.model, **batch)  # Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ
                    outputs = self.model(**batch)
                    loss = self.criterion(
                        outputs.logits.view(-1, outputs.logits.size(-1)),
                        batch["labels"].view(-1)
                    )

                # Ê†πÊçÆ‰º†ÂÖ•ÂèÇÊï∞ÁöÑÁ±ªÂûãÈÄÇÈÖçÔºöÂ≠óÂÖ∏ÊàñÂàóË°®/ÂÖÉÁªÑ
                # ÈÄâÊã©ÈúÄË¶ÅËÆ°ÁÆó Hessian ÁöÑÂèÇÊï∞
                if isinstance(param_list, dict):
                    params = [p for p in param_list.values()
                              if p.requires_grad]
                elif isinstance(param_list, (list, tuple)):
                    params = [p for p in param_list if p.requires_grad]
                else:
                    params = []  # Ëã•‰º†ÂÖ•‰∏∫ None ÊàñÂÖ∂ÂÆÉÁ±ªÂûãÔºåÂàôÁ©∫Â§ÑÁêÜ

                # ËÆ°ÁÆó‰∏ÄÈò∂Ê¢ØÂ∫¶
                grads = torch.autograd.grad(loss, params, create_graph=True)

                def hvp_func(v):
                    """ËÆ°ÁÆó Hessian ÂêëÈáèÁßØ,Èó≠ÂåÖÂáΩÊï∞‰øùÊåÅËÆ°ÁÆóÂõæ"""

                    split_sizes = [p.numel()
                                   for p in params]  # ËÆ°ÁÆóÊØè‰∏™ param ÂØπÂ∫îÁöÑÂ§ßÂ∞è
                    v_split = torch.split(v, split_sizes)  # ÊåâÁÖßÊØè‰∏™ÂèÇÊï∞ÁöÑÂΩ¢Áä∂ÊãÜÂàÜ v
                    v_reshaped = [v_i.view(p.shape) for v_i, p in zip(
                        v_split, params)]  # ÈáçÊñ∞Ë∞ÉÊï¥ v_i ÂΩ¢Áä∂

                    # logger.info(f"grads shape: {[g.shape for g in grads]}")
                    # logger.info(f"v shape: {v.shape}")
                    # logger.info(f"Total params count: {len(params)}, Total elements in params: {sum(split_sizes)}")
                    # logger.info(f"üîπ First 5 split sizes: {split_sizes[:5]}")
                    # logger.info(f"üîπ First 5 param shapes: {[p.shape for p in params[:5]]}")
                    # logger.info(f"üîπ First 5 v_split shapes: {[v_i.shape for v_i in v_split[:5]]}")

                    # ËÆ°ÁÆó Hessian ‰ΩúÁî®
                    Hv = torch.autograd.grad(
                        grads, params, grad_outputs=v_reshaped,
                        retain_graph=True, allow_unused=True
                    )
                    # modified
                    # Hv = torch.autograd.grad(
                    #     grads, params, grad_outputs=v_reshaped,
                    #     retain_graph=False, allow_unused=True
                    # )

                    # ÊãºÊé•ËÆ°ÁÆóÁªìÊûú
                    Hv_flattened = torch.cat([
                        hv.contiguous().flatten() if hv is not None else torch.zeros_like(p).flatten()
                        for hv, p in zip(Hv, params)
                    ]).to(self.device)

                    # logger.info(f"üîπ Hv computed successfully, shape: {Hv_flattened.shape}")
                    torch.cuda.empty_cache()  # ÈáäÊîæÊòæÂ≠ò
                    return Hv_flattened

                return hvp_func

            # def lanczos_algorithm(self, hvp_func, dim, order=5, num_splits=4, random_seed=0):
                """
                ‰ΩøÁî®Ê†áÂáÜ Lanczos ÊñπÊ≥ïËÆ°ÁÆó Hessian ÁâπÂæÅÂÄºÔºåÂπ∂Âü∫‰∫é v_chunks ÂàÜÂùóËÆ°ÁÆó
                - hvp_func: Hessian-Vector Product ËÆ°ÁÆóÂáΩÊï∞
                - dim: ÈúÄË¶ÅËÆ°ÁÆóÁöÑÂèÇÊï∞Áª¥Â∫¶
                - order: Lanczos Ëø≠‰ª£Èò∂Êï∞
                - num_splits: Â∞Ü v ÊãÜÂàÜÊàê num_splits ‰ªΩÔºåÈÄêÊ≠•ËÆ°ÁÆó HVP ‰ª•ÂáèÂ∞ëÊòæÂ≠òÂç†Áî®
                - random_seed: ÈöèÊú∫ÁßçÂ≠ê
                """
                torch.manual_seed(random_seed)
                warnings.filterwarnings("ignore", category=UserWarning)

                # ÈôêÂà∂ Hessian ËÆ°ÁÆóÁöÑÊúÄÂ§ßÁª¥Â∫¶
                dim = min(dim, self.max_dim)

                # ‰ΩøÁî® `float16` Èôç‰ΩéÊòæÂ≠òÈúÄÊ±Ç
                tridiag = torch.zeros(
                    (order, order), dtype=torch.float16, device=self.device)
                vecs = torch.zeros(
                    (dim, order), dtype=torch.float16, device=self.device)

                # ÁîüÊàêÂàùÂßãÈöèÊú∫ÂêëÈáèÂπ∂ÂΩí‰∏ÄÂåñ
                init_vec = torch.randn(
                    dim, 1, dtype=torch.float16, device=self.device)
                init_vec /= torch.norm(init_vec)
                vecs[:, 0:1] = init_vec

                beta = 0
                v_old_chunks = [torch.zeros_like(chunk) for chunk in torch.chunk(
                    init_vec, num_splits, dim=0)]  # Â≠òÂÇ®‰∏ä‰∏ÄÊ¨°ÁöÑ v_old

                for i in range(order):
                    start_time = time.time()
                    v = vecs[:, i:i+1]

                    # ‚úÖ ÂàÜÂùóËÆ°ÁÆó HVP
                    v_chunks = torch.chunk(v, num_splits, dim=0)
                    w_chunks = []

                    for j, v_chunk in enumerate(v_chunks):
                        w_chunk = hvp_func(v_chunk)  # ËÆ°ÁÆó HVP
                        w_chunk = w_chunk - beta * \
                            v_old_chunks[j]  # ËÆ°ÁÆó w - beta * v_old
                        w_chunks.append(w_chunk)

                    # ‚úÖ ËÆ°ÁÆó alphaÔºàÂàÜÂùóÔºâ
                    alpha_chunks = [torch.matmul(
                        w_chunk.T, v_chunk) for w_chunk, v_chunk in zip(w_chunks, v_chunks)]
                    alpha = sum(alpha_chunks)  # ÈÄêÂùóÁ¥ØÂä†
                    tridiag[i, i] = alpha

                    # ‚úÖ ËÆ°ÁÆó w_chunks = w_chunks - alpha * v_chunks
                    for j in range(num_splits):
                        w_chunks[j] = w_chunks[j] - alpha * v_chunks[j]

                    # ‚úÖ ÈáçÊ≠£‰∫§ÂåñÔºàÂàÜÂùóËÆ°ÁÆóÔºâ
                    for j in range(i):
                        tau_chunks = torch.chunk(
                            vecs[:, j:j+1], num_splits, dim=0)
                        coeff_chunks = [torch.matmul(
                            w_chunk.T, tau_chunk) for w_chunk, tau_chunk in zip(w_chunks, tau_chunks)]
                        coeff = sum(coeff_chunks)

                        for k in range(num_splits):
                            w_chunks[k] = w_chunks[k] - coeff * tau_chunks[k]

                    # ‚úÖ ÈáçÊñ∞ËÆ°ÁÆó beta
                    beta = torch.norm(torch.cat(w_chunks, dim=0))
                    if beta < 1e-6:
                        warnings.warn(
                            f"Êï∞ÂÄºÁ®≥ÂÆöÊÄßÈóÆÈ¢ò: beta={beta.item()} Âú®Ëø≠‰ª£ {i} Êó∂ËøáÂ∞è„ÄÇ")

                    # ‚úÖ Êõ¥Êñ∞ vecsÔºàÁõ¥Êé•‰ΩøÁî® w_chunksÔºâ
                    if i + 1 < order:
                        tridiag[i, i+1] = beta
                        tridiag[i+1, i] = beta
                        vecs[:, i+1:i+2] = torch.cat(
                            [w_chunks[j] / beta for j in range(num_splits)], dim=0)

                    # ‚úÖ Êõ¥Êñ∞ v_old_chunksÔºàÁî®‰∫é‰∏ãËΩÆËø≠‰ª£Ôºâ
                    v_old_chunks = [
                        w_chunks[j].clone() / beta for j in range(num_splits)]

                    elapsed_time = time.time() - start_time
                    print(
                        f"Iter {i}/{order}: Œ±={alpha.item():.6f}, Œ≤={beta.item():.6f}, Time={elapsed_time:.2f}s")

                    torch.cuda.empty_cache()  # ÈáäÊîæÊòæÂ≠ò

                return tridiag

            def block_lanczos(self, hvp_func, dim, k=10, block_size=4):
                """
                ÊîπËøõÁÇπ7ÔºöÂàÜÂùóLanczosÁÆóÊ≥ïÔºàÂÜÖÂ≠ò‰ºòÂåñÔºâ
                ÂÖ∂‰∏≠Ë∂ÖÂèÇÊï∞ k ÊòØËø≠‰ª£Ê¨°Êï∞Ôºåblock_size ÊòØÂùóÁöÑÂ§ßÂ∞è„ÄÇ

                """
                logger.info(f'***5***--5-3**5 begin block_lanczos() ')
                # ÂàùÂßãÂåñÂàÜÂùóÊ≠£‰∫§Âü∫
                Q = torch.zeros((k+1)*block_size, dim, device=self.device)
                T = torch.zeros(k*block_size, k*block_size, device=self.device)

                # ÁîüÊàêÂàùÂßãÂàÜÂùó
                V = torch.randn(dim, block_size, device=self.device)
                V, _ = torch.linalg.qr(V)  # Ê≠£‰∫§Âåñ
                Q[:block_size] = V.T

                for i in range(k):
                    start_idx = i * block_size
                    # ËÆ°ÁÆóHessian‰ΩúÁî®
                    HV = torch.stack([hvp_func(Q[start_idx + j])
                                     for j in range(block_size)])

                    # Ê≠£‰∫§ÂåñËøáÁ®ã
                    for j in range(start_idx, start_idx + block_size):
                        T[j, :j+1] = Q[:j+1] @ HV[j-start_idx]  # ËÆ°ÁÆó‰∏âÂØπËßíÁü©Èòµ T
                        # HV[j-start_idx] -= Q[:j+1] @ T[j, :j+1].T
                        # HV[j-start_idx] -= Q[:j+1] @ T[j, :j+1].unsqueeze(1)  # ‚úÖ ‰øÆÊ≠£ÂΩ¢Áä∂ÈóÆÈ¢ò
                        # HV[j-start_idx] -= Q[:j+1].T @ T[j, :j+1].unsqueeze(1)  # ‚úÖ ‰øÆÊ≠£ÂΩ¢Áä∂ÈóÆÈ¢ò
                        # üîπ Âú®ËÆ°ÁÆóÂâçÊ£ÄÊü•ÂΩ¢Áä∂
                        # logger.info(f"üîπ Q[:j+1].shape: {Q[:j+1].shape}")
                        # logger.info(f"üîπ Q[:j+1].T.shape: {Q[:j+1].T.shape}")
                        # logger.info(f"üîπ T[j, :j+1].shape: {T[j, :j+1].shape}")
                        # logger.info(f"üîπ T[j, :j+1].unsqueeze(1).shape: {T[j, :j+1].unsqueeze(1).shape}")
                        # logger.info(f"üîπ HV[j-start_idx].shape: {HV[j-start_idx].shape}")

                        # ‚úÖ ‰øÆÊ≠£ÂΩ¢Áä∂
                        HV[j-start_idx] -= (Q[:j+1].T @
                                            T[j, :j+1].unsqueeze(1)).squeeze()

                    # QRÂàÜËß£
                    V, R = torch.linalg.qr(HV.T)  # Ê≠£‰∫§Âåñ
                    Q[start_idx+block_size:start_idx+2*block_size] = V.T

                    # ‚úÖ ‰øÆÊ≠£ÈîôËØØÔºöÁ°Æ‰øùÁ¥¢ÂºïËåÉÂõ¥‰∏ç‰ºö‰∏∫Á©∫
                    if start_idx+block_size < T.shape[0]:
                        end_row = min(start_idx+2*block_size, T.shape[0])
                        end_col = min(start_idx+block_size, T.shape[1])

                        # logger.info(f"üîπ Updating T matrix at [{start_idx+block_size}:{end_row}, {start_idx}:{end_col}]")

                        T[start_idx+block_size:end_row, start_idx:end_col] = R.T
                    else:
                        logger.info(
                            f"‚ùå Skipping T update at [{start_idx+block_size}:{end_row}, {start_idx}:{end_col}] to prevent empty slice.")

                    # # ‚úÖ Á°Æ‰øù T ‰∏ç‰ºöË∂äÁïå
                    # end_row = min(start_idx+2*block_size, T.shape[0])
                    # end_col = min(start_idx+block_size, T.shape[1])

                    # logger.info(f"üîπ Updating T matrix at [{start_idx+block_size}:{end_row}, {start_idx}:{end_col}]")

                    # T[start_idx+block_size:end_row, start_idx:end_col] = R.T
                    # T[start_idx+block_size:start_idx+2*block_size,
                    # start_idx:start_idx+block_size] = R.T

                # ËÆ°ÁÆóÁâπÂæÅÂÄº
                T_np = T.cpu().numpy()
                eigvals = np.linalg.eigvalsh(T_np)
                return eigvals[-block_size:]  # ËøîÂõûÊúÄÂ§ßÁâπÂæÅÂÄº

        # --------------------- ‰∏ªËÆ°ÁÆóÊµÅÁ®ã ---------------------
        logger.info(f'***5***--5-3**3 begin main loop   ')

        # Âè™Â≠òÂÇ®ÂàùÂßãÁä∂ÊÄÅÔºà‰∏çÂ∏¶Ê¢ØÂ∫¶Ôºâ,Áî®‰∫éÊÅ¢Â§çÊ®°ÂûãÁä∂ÊÄÅ
        original_params_to_calculate_hessian = {}

        if Flag_Nlora_newtask and (not Flag_Nlora_full):
            dom_eigs_Nlora_tasklora = []
            Nlora_params_tasklora = {}
            # Nlora_params_lora = {}
            for name, param in model.named_parameters():
                if name.find("loranew_") != -1:
                    # ÂΩì‰ΩøÁî®Nlora ÊñπÊ≥ïÊó∂ÔºåÈúÄË¶ÅÂØπlora_ Âíå loranew_ ËøõË°åÂå∫ÂàÜ
                    # ËøõË°åÊâ∞Âä®Âè™ËÄÉËôë loranew ÁöÑÈÉ®ÂàÜÔºåÂç≥Âè™Êõ¥Êñ∞  ‰∏é‰ªªÂä°ÊúâÂÖ≥ÁöÑÈÇ£‰∏ÄÈÉ®ÂàÜ lora
                    Nlora_params_tasklora[name] = param
                    original_params_to_calculate_hessian[name] = param.data.clone(
                    )

        elif Flag_Nlora_newtask and (Flag_Nlora_full):
            dom_eigs_Nlora_lora = []
            Nlora_params_lora = {}
            for name, param in model.named_parameters():
                if name.find("loranew_") != -1:
                    # ÂΩì‰ΩøÁî®Nlora ÊñπÊ≥ïÊó∂ÔºåÈúÄË¶ÅÂØπlora_ Âíå loranew_ ËøõË°åÂå∫ÂàÜ
                    Nlora_params_lora[name] = param
                    original_params_to_calculate_hessian[name] = param.data.clone(
                    )
                elif name.find("lora_") != -1:
                    # ÂΩì‰ΩøÁî®lora ÊñπÊ≥ïÊó∂ÔºåÂè™Êúâ‰∏Ä‰∏™ loraÁöÑÈÉ®ÂàÜ ËøõË°åÊâ∞Âä®Âè™ËÄÉËôë lora ÁöÑÈÉ®ÂàÜÔºåÂç≥Âè™Êõ¥Êñ∞
                    Nlora_params_lora[name] = param
                    original_params_to_calculate_hessian[name] = param.data.clone(
                    )

        elif Flag_lora:
            dom_eigs_lora = []
            # Ëé∑ÂèñÂèÇÊï∞ÈõÜÂêàÔºàÊîπËøõÁÇπ8ÔºöÂä®ÊÄÅÂèÇÊï∞Â§ÑÁêÜÔºâ
            lora_params = {}
            for name, param in model.named_parameters():
                if name.find("lora_") != -1:
                    lora_params[name] = param
                    original_params_to_calculate_hessian[name] = param.data.clone(
                    )

        calculator = HessianCalculator(model, device)

        # ÂàÜÂ∏ÉÂºèÈÄö‰ø°ÂàùÂßãÂåñÔºàÊîπËøõÁÇπ9ÔºöÂàÜÂ∏ÉÂºèÊîØÊåÅÔºâ

        if torch.distributed.is_initialized():
            logger.info(f'***5***--5-3**4 distribute--yes ')
            world_size = torch.distributed.get_world_size()
            rank = torch.distributed.get_rank()
            all_batches = all_batches[rank::world_size]  # Êï∞ÊçÆÂàÜÁâá
            logger.info(f'***5***--5-3**4 {len(all_batches)} ')
        else:
            logger.info(f'***5***--5-3**4 distribute--no ')
            world_size = 1
            rank = 0

        try:
            for batch in tqdm(all_batches, desc=f"Rank {rank}: Processing"):
                # ÈáçÁΩÆÊ®°ÂûãÂèÇÊï∞ÔºàÂÖ≥ÈîÆÊîπËøõÁÇπ10ÔºöÊ∂àÈô§ÂèÇÊï∞Ê±°ÊüìÔºâ
                # model.load_state_dict(original_state)
                # ÊÅ¢Â§çÂèÇÊï∞Êó∂‰ªÖÊìç‰ΩúÈúÄË¶Å‰øÆÊîπÁöÑÈÉ®ÂàÜÔºà‰ºòÂåñÁÇπ1Ôºâ
                for name in original_params_to_calculate_hessian:
                    model.state_dict()[name].copy_(
                        original_params_to_calculate_hessian[name])

                # ËÆ°ÁÆóÂÖ®Ê®°ÂûãHessian
                # logger.info(f'***5***--5-3**5 begin calculate Hessian ')
                # hvp_full = calculator.compute_hvp(batch)
                # eigvals = calculator.block_lanczos(hvp_full, dim=sum(p.numel() for p in full_params))
                # dom_eigs_full.extend(eigvals.tolist())

                # ------------full model paramter analyase Hessien
                # saved_flags = {}
                # for name, param in calculator.model.named_parameters():
                #     # saved_flags‰øùÂ≠ò‰πãÂâçÁöÑÁ•ûÁªèÁΩëÁªúÁöÑ requires_grad Áä∂ÊÄÅ
                #     saved_flags[name] = param.requires_grad
                #      # ÂØπÂÖ®Ê®°Âûã HessianÔºö‰∏¥Êó∂ÊøÄÊ¥ªÊâÄÊúâÂèÇÊï∞
                #     param.requires_grad = True

                # ---Ê†πÊçÆsaved_flags ÊÅ¢Â§ç‰πãÂâçÁöÑÁä∂ÊÄÅ
                # for name, param in calculator.model.named_parameters():
                #     param.requires_grad = saved_flags.get(name, param.requires_grad)

                # ------------full model paramter analyase Hessien
                # hvp_full = compute_batch_hvp(batch, full_params)
                # eigvals_full = lanczos_iteration(hvp_full, full_params, k=num_iter)
                # dom_eigs_full.append(eigvals_full.max())
                # restore_gradients(model, saved_flags)

                # ËÆ°ÁÆóLoRA Hessian
                if Flag_Nlora_newtask and (not Flag_Nlora_full):

                    logger.info(
                        f'***5***--5-3**Model device: {next(model.parameters()).device},Batch device: {next(iter(batch.values())).device} ')
                    logger.info(
                        f"Type of lora_params: {type(Nlora_params_tasklora)}")
                    # logger.info(f"Example entry in lora_params: {list(lora_params.items())[:5]}")  # Âè™ÊâìÂç∞Ââç5‰∏™
                    logger.info(
                        f"Type of original_params_to_calculate_hessian: {type(original_params_to_calculate_hessian)}")
                    # logger.info(f"Example original_params_to_calculate_hessian: {list(original_params_to_calculate_hessian.items())[:5]}")  # Âè™ÊâìÂç∞Ââç5‰∏™

                    hvp_Nlora_tasklora = calculator.compute_hvp(
                        batch, Nlora_params_tasklora)
                    eigvals = calculator.block_lanczos(hvp_Nlora_tasklora, dim=sum(
                        p.numel() for p in Nlora_params_tasklora.values()))
                    dom_eigs_Nlora_tasklora.extend(eigvals.tolist())

                    # hvp_Nlora_lora = calculator.compute_hvp(batch,Nlora_params_lora)
                    # eigvals = calculator.block_lanczos(hvp_Nlora_lora, dim=sum(p.numel() for p in Nlora_params_fulllor))
                    # dom_eigs_Nlora_lora.extend(eigvals.tolist())

                # ËÆ°ÁÆóLoRA Hessian ËøôÈáåËÆ°ÁÆóÁöÑÊòØÊâÄÊúâloraÁõ∏ÂÖ≥ÈÉ®ÂàÜ
                if Flag_Nlora_newtask and (Flag_Nlora_full):

                    logger.info(
                        f'***5***--5-3**Model device: {next(model.parameters()).device},Batch device: {next(iter(batch.values())).device} ')
                    logger.info(
                        f"Type of lora_params: {type(Nlora_params_tasklora)}")
                    # logger.info(f"Example entry in lora_params: {list(lora_params.items())[:5]}")  # Âè™ÊâìÂç∞Ââç5‰∏™
                    logger.info(
                        f"Type of original_params_to_calculate_hessian: {type(original_params_to_calculate_hessian)}")
                    # logger.info(f"Example original_params_to_calculate_hessian: {list(original_params_to_calculate_hessian.items())[:5]}")  # Âè™ÊâìÂç∞Ââç5‰∏™

                    # hvp_Nlora_tasklora = calculator.compute_hvp(batch,Nlora_params_tasklora)
                    # eigvals = calculator.block_lanczos(hvp_Nlora_tasklora, dim=sum(p.numel() for p in Nlora_params_tasklora.values()))
                    # dom_eigs_Nlora_tasklora.extend(eigvals.tolist())

                    hvp_Nlora_lora = calculator.compute_hvp(
                        batch, Nlora_params_lora)
                    eigvals = calculator.block_lanczos(hvp_Nlora_lora, dim=sum(
                        p.numel() for p in Nlora_params_lora.values()))
                    dom_eigs_Nlora_lora.extend(eigvals.tolist())

                if Flag_lora:
                    logger.info(
                        f'***5***--5-3**Model device: {next(model.parameters()).device},Batch device: {next(iter(batch.values())).device} ')
                    logger.info(f"Type of lora_params: {type(lora_params)}")
                    # logger.info(f"Example entry in lora_params: {list(lora_params.items())[:5]}")  # Âè™ÊâìÂç∞Ââç5‰∏™
                    logger.info(
                        f"Type of original_params_to_calculate_hessian: {type(original_params_to_calculate_hessian)}")
                    # logger.info(f"Example original_params_to_calculate_hessian: {list(original_params_to_calculate_hessian.items())[:5]}")  # Âè™ÊâìÂç∞Ââç5‰∏™

                    hvp_lora = calculator.compute_hvp(batch, lora_params)
                    eigvals = calculator.block_lanczos(
                        hvp_lora, dim=sum(p.numel() for p in lora_params.values()))
                    dom_eigs_lora.extend(eigvals.tolist())
                    # tridiag = calculator.lanczos_algorithm(hvp_lora, dim=sum(p.numel() for p in lora_params.values()))
                    # dom_eigs_lora.extend(torch.linalg.eigvalsh(tridiag).tolist())

                # ÂÜÖÂ≠òÊ∏ÖÁêÜ
                torch.cuda.empty_cache()

        except RuntimeError as e:
            logger.error(f"HessianËÆ°ÁÆóÂ§±Ë¥•: {str(e)}")
            if "CUDA out of memory" in str(e):
                logger.warning("Â∞ùËØïÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ...")
                # Ê≠§Â§ÑÂèØÊ∑ªÂä†fallbackÈÄªËæë
            raise

        # --------------------- ÁªìÊûúÂ§ÑÁêÜ‰∏é‰øùÂ≠ò ---------------------
        logger.info(f'***5***--5-3**6 begin save ')
        # ÂàÜÂ∏ÉÂºèÁªìÊûúËÅöÂêàÔºàÊîπËøõÁÇπ11Ôºâ
        if torch.distributed.is_initialized():
            world_size = torch.distributed.get_world_size()
            if Flag_Nlora_newtask and (not Flag_Nlora_full):
                dom_eigs_Nlora_tasklora_tensor = torch.tensor(
                    dom_eigs_Nlora_tasklora, device=device)
                # dom_eigs_Nlora_fulllora_tensor = torch.tensor(dom_eigs_Nlora_lora, device=device)

                # ÂàõÂª∫Êé•Êî∂ÊâÄÊúâËøõÁ®ãÊï∞ÊçÆÁöÑÂàóË°®
                all_dom_eigs_Nlora_tasklora = [torch.zeros_like(
                    dom_eigs_Nlora_tasklora_tensor) for _ in range(world_size)]
                # all_dom_eigs_Nlora_fulllora = [torch.zeros_like(dom_eigs_Nlora_fulllora_tensor) for _ in range(world_size)]

                # Êî∂ÈõÜÊâÄÊúâËøõÁ®ãÁöÑÊï∞ÊçÆ
                torch.distributed.all_gather(
                    all_dom_eigs_Nlora_tasklora, dom_eigs_Nlora_tasklora_tensor)
                # torch.distributed.all_gather(all_dom_eigs_Nlora_fulllora, dom_eigs_Nlora_fulllora_tensor)

                # ÊãºÊé•ÊâÄÊúâÊî∂ÈõÜÂà∞ÁöÑÊï∞ÊçÆ
                dom_eigs_Nlora_tasklora = torch.cat(
                    all_dom_eigs_Nlora_tasklora, dim=0).cpu().numpy().tolist()
                # dom_eigs_Nlora_fulllora = torch.cat(all_dom_eigs_Nlora_fulllora, dim=0).cpu().numpy().tolist()
                # ËÆ°ÁÆóÁªüËÆ°ÊåáÊ†á
                stats_Nlora_tasklora = self._compute_stats(
                    dom_eigs_Nlora_tasklora)
                # stats_Nlora_lora = self._compute_stats(dom_eigs_Nlora_fulllora)

            elif Flag_Nlora_newtask and (Flag_Nlora_full):
                # dom_eigs_Nlora_tasklora_tensor = torch.tensor(dom_eigs_Nlora_tasklora, device=device)
                dom_eigs_Nlora_fulllora_tensor = torch.tensor(
                    dom_eigs_Nlora_lora, device=device)

                # ÂàõÂª∫Êé•Êî∂ÊâÄÊúâËøõÁ®ãÊï∞ÊçÆÁöÑÂàóË°®
                # all_dom_eigs_Nlora_tasklora = [torch.zeros_like(dom_eigs_Nlora_tasklora_tensor) for _ in range(world_size)]
                all_dom_eigs_Nlora_fulllora = [torch.zeros_like(
                    dom_eigs_Nlora_fulllora_tensor) for _ in range(world_size)]

                # Êî∂ÈõÜÊâÄÊúâËøõÁ®ãÁöÑÊï∞ÊçÆ
                # torch.distributed.all_gather(all_dom_eigs_Nlora_tasklora, dom_eigs_Nlora_tasklora_tensor)
                torch.distributed.all_gather(
                    all_dom_eigs_Nlora_fulllora, dom_eigs_Nlora_fulllora_tensor)

                # ÊãºÊé•ÊâÄÊúâÊî∂ÈõÜÂà∞ÁöÑÊï∞ÊçÆ
                # dom_eigs_Nlora_tasklora = torch.cat(all_dom_eigs_Nlora_tasklora, dim=0).cpu().numpy().tolist()
                dom_eigs_Nlora_fulllora = torch.cat(
                    all_dom_eigs_Nlora_fulllora, dim=0).cpu().numpy().tolist()
                # ËÆ°ÁÆóÁªüËÆ°ÊåáÊ†á
                # stats_Nlora_tasklora = self._compute_stats(dom_eigs_Nlora_tasklora)
                stats_Nlora_lora = self._compute_stats(dom_eigs_Nlora_fulllora)

            elif Flag_lora:
                # dom_eigs_lora = torch.tensor(dom_eigs_lora, device=device)
                # torch.distributed.all_reduce(dom_eigs_lora)
                # dom_eigs_lora = dom_eigs_lora.cpu().numpy().tolist()
                dom_eigs_lora_tensor = torch.tensor(
                    dom_eigs_lora, device=device)

                # ÂàõÂª∫Êé•Êî∂ÊâÄÊúâËøõÁ®ãÊï∞ÊçÆÁöÑÂàóË°®
                all_dom_eigs_lora = [torch.zeros_like(
                    dom_eigs_lora_tensor) for _ in range(world_size)]

                # Êî∂ÈõÜÊâÄÊúâËøõÁ®ãÁöÑÊï∞ÊçÆ
                torch.distributed.all_gather(
                    all_dom_eigs_lora, dom_eigs_lora_tensor)

                # ÊãºÊé•ÊâÄÊúâÊî∂ÈõÜÂà∞ÁöÑÊï∞ÊçÆ
                dom_eigs_lora = torch.cat(
                    all_dom_eigs_lora, dim=0).cpu().numpy().tolist()

                # ËÆ°ÁÆóÁªüËÆ°ÊåáÊ†á
                stats_lora = self._compute_stats(dom_eigs_lora)

        # HDF5‰øùÂ≠òÔºàÊîπËøõÁÇπ12ÔºöÂÖÉÊï∞ÊçÆÂ¢ûÂº∫Ôºâ
        # ‰ªÖ rank=0 ËøõÁ®ã‰øùÂ≠òÁªìÊûúÔºåÈÅøÂÖçÂ§ö‰∏™ËøõÁ®ãÂêåÊó∂ÂÜôÂÖ• HDF5
        if rank == 0:
            if Flag_Nlora_newtask and (not Flag_Nlora_full):
                # with h5py.File(hessian_file_Nlora_full, "w") as hf:
                #     hf.attrs["created_at"] = datetime.now().isoformat()
                #     hf.attrs["model_type"] = type(model).__name__
                #     for k, v in stats_Nlora_lora.items():
                #         hf.create_dataset(k, data=v)
                #     hf.create_dataset("dominant_eigs", data=np.array(dom_eigs_Nlora_fulllora))

                with h5py.File(hessian_file_Nlora_tasklora, "w") as hf:
                    hf.attrs["created_at"] = datetime.now().isoformat()
                    hf.attrs["model_type"] = type(model).__name__
                    for k, v in stats_Nlora_tasklora.items():
                        hf.create_dataset(k, data=v)
                    hf.create_dataset("dominant_eigs", data=np.array(
                        dom_eigs_Nlora_tasklora))

                logger.info(f"ËÆ°ÁÆóÂÆåÊàêÔºåÁªìÊûú‰øùÂ≠òËá≥ {hessian_file_Nlora_tasklora} Êñá‰ª∂")

            elif Flag_Nlora_newtask and (Flag_Nlora_full):
                with h5py.File(hessian_file_Nlora_fulllora, "w") as hf:
                    hf.attrs["created_at"] = datetime.now().isoformat()
                    hf.attrs["model_type"] = type(model).__name__
                    for k, v in stats_Nlora_tasklora.items():
                        hf.create_dataset(k, data=v)
                    hf.create_dataset(
                        "dominant_eigs", data=np.array(dom_eigs_Nlora_lora))

                logger.info(f"ËÆ°ÁÆóÂÆåÊàêÔºåÁªìÊûú‰øùÂ≠òËá≥ {hessian_file_Nlora_fulllora} Êñá‰ª∂")

            elif Flag_lora:
                with h5py.File(hessian_file_lora, "w") as hf:
                    hf.attrs["created_at"] = datetime.now().isoformat()
                    hf.attrs["model_type"] = type(model).__name__
                    for k, v in stats_lora.items():
                        hf.create_dataset(k, data=v)
                    hf.create_dataset(
                        "dominant_eigs", data=np.array(dom_eigs_lora))

                logger.info(f"ËÆ°ÁÆóÂÆåÊàêÔºåÁªìÊûú‰øùÂ≠òËá≥ {hessian_file_lora} Êñá‰ª∂")
        # üöÄ Ê∑ªÂä†ËøõÁ®ãÂêåÊ≠• & ÂÖ≥Èó≠ÂàÜÂ∏ÉÂºèËøõÁ®ã
        if torch.distributed.is_initialized():
            logger.info("ÊâÄÊúâËøõÁ®ãÂêåÊ≠•‰∏≠...")
            torch.distributed.barrier()  # Á°Æ‰øùÊâÄÊúâËøõÁ®ãÈÉΩÂÆåÊàêÂÜçÁªßÁª≠

            if torch.distributed.get_rank() == 0:
                logger.info("ÊâÄÊúâËøõÁ®ãÂ∑≤ÂÆåÊàêËÆ°ÁÆóÔºåÂºÄÂßãÂÖ≥Èó≠ÂàÜÂ∏ÉÂºèËøõÁ®ã...")

            torch.distributed.destroy_process_group()  # ÈáäÊîæ NCCL ËµÑÊ∫ê
            logger.info("ÂàÜÂ∏ÉÂºèËøõÁ®ãÂ∑≤Ê≠£Á°ÆÂÖ≥Èó≠")
        return True
